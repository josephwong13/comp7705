{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      spam                                            content\n",
      "0        1  urgent! call 09061749602 from landline. your c...\n",
      "1        1  +449071512431 urgent! this is the 2nd attempt ...\n",
      "2        1  free for 1st week! no1 nokia tone 4 ur mob eve...\n",
      "3        1  urgent! call 09066612661 from landline. your c...\n",
      "4        1  winner!! as a valued network customer you have...\n",
      "...    ...                                                ...\n",
      "5881     0                ok lor ?_ reaching then message me.\n",
      "5882     1  marvel mobile play the official ultimate spide...\n",
      "5883     0             it???s reassuring in this crazy world.\n",
      "5884     1  asked 3mobile if 0870 chatlines inclu in free ...\n",
      "5885     0              will ?_ b going to esplanade fr home?\n",
      "\n",
      "[5886 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# set_01 is merged by below samples, by a .net program\n",
    "# all of them are english messages\n",
    "# https://github.com/semnan-university-ai/SMS-Spam-Collection/blob/main/english_big.txt\n",
    "# https://github.com/rodrigodelmonte/pytext-lab/blob/master/dataset/smsspam_train.tsv\n",
    "# https://github.com/rodrigodelmonte/pytext-lab/blob/master/dataset/smsspam_test.tsv\n",
    "# https://github.com/Peviroy/MailChecker/blob/master/data/spam.csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "UTF_8 = 'utf-8'\n",
    "\n",
    "# read csv, no header\n",
    "df1 = pd.read_csv('../dataset/set_01/set_01.csv', header=None, encoding=UTF_8)\n",
    "\n",
    "# define header\n",
    "df1.rename(columns={0: 'spam', 1: 'content'}, inplace=True)\n",
    "\n",
    "# transform content to lower case before any further process\n",
    "df1.content = df1.content.str.lower()\n",
    "\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "亲爱的chankwan pok先生或女士，欢迎您加入“心享”计划 hpr”，您的会员编码为：111023621183，初始密码为：006653。恭喜您可尊享通过“中旅酒店心享会”公众号推荐朋友加入会员活动，被推荐的新会员还可以领取代金券礼包！详询400-669-0000（大陆地区）、852-36040000（港澳地区）。\n",
      "親愛的chankwan pok先生或女士，歡迎您加入「心享」計劃 hpr」，您的會員編碼為：111023621183，初始密碼為：006653。恭喜您可尊享通過「中旅酒店心享會」公眾號推薦朋友加入會員活動，被推薦的新會員還可以領取代金券禮包！詳詢400-669-0000（大陸地區）、852-36040000（港澳地區）。\n",
      "【阿里巴巴】验证码798505，您正在登录验证，切勿将验证码泄露于他人，验证码15分钟内有效。\n",
      "【阿里巴巴】驗證碼798505，您正在登錄驗證，切勿將驗證碼洩露於他人，驗證碼15分鐘內有效。\n",
      "     spam                                            content\n",
      "0       1    [netflix] : 無法處理你的自動付款。你的帳戶將被禁用。t.co/ntakhpfwqr\n",
      "1       1  您好！我系橙橙\\n邀請您加細妹微信：76169639 睇朋友圈保有您喜歡嘅哦！\\n有國際大牌...\n",
      "2       1  您好！{本店支持淘寶店鋪下單，淘寶店鋪付款}\\n請加我微信（wechat）: 1198632...\n",
      "3       1  aeon: 即日起至3月5日，登入「aeon 香港」手機應用程式或aeon網上客戶服務，申請...\n",
      "4       1  恒生hang seng：信用卡「現金分期」計劃：\\n已為你預先批核多一筆現金，你的稅季限定個...\n",
      "..    ...                                                ...\n",
      "495     0                             327886 是你的moneyhero驗證碼\n",
      "496     0  nti has received your order\\n[services redeeme...\n",
      "497     0  渣打香港:您的渣打信用卡結尾8492於07/10 在 mhk restaura 有一項hkd...\n",
      "498     0  uk visas and immigration home office: your cod...\n",
      "499     0                    使用驗證碼 850663 進行 aia connect 驗證。\n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# set_02 is some samples inputted by our team member, through a google form\n",
    "\n",
    "import re\n",
    "from zhconv import convert\n",
    "\n",
    "\n",
    "def scToTc(text):\n",
    "    text = convert(text, 'zh-tw')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# read csv, get two columns\n",
    "df2 = pd.read_csv('../dataset/set_02/SMS collect form (Responses) - Form Responses 1.csv', header=0, usecols=['Content', 'Spam or Ham'], encoding=UTF_8)\n",
    "\n",
    "# rename and reorder columns\n",
    "df2 = df2.rename(columns={'Content': 'content', 'Spam or Ham': 'spam'})\n",
    "df2 = df2.reindex(columns=['spam', 'content'])\n",
    "\n",
    "# Spam = 1, Ham = 0\n",
    "df2.spam = df2.spam.map({'Spam': 1, 'Ham': 0})\n",
    "\n",
    "# transform content to lower case before any further process\n",
    "df2.content = df2.content.str.lower()\n",
    "\n",
    "# for testing only\n",
    "print(re.sub(r'[\\r\\n]', ' ', df2.content[26]))\n",
    "print(re.sub(r'[\\r\\n]', ' ', scToTc(df2.content[26])))\n",
    "print(re.sub(r'[\\r\\n]', ' ', df2.content[146]))\n",
    "print(re.sub(r'[\\r\\n]', ' ', scToTc(df2.content[146])))\n",
    "\n",
    "# transform content to traditional chinese (taiwan)\n",
    "df2.content = df2.content.map(scToTc)\n",
    "\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       spam                                            content\n",
      "0         1  官網認證日入官網認證日入官網認證日入官網認證日入官網認證日入官網認證日入官網認證日入官網認證...\n",
      "1         1  我有芳罰跟著芳法操作不愁沒克源而且都是靜準顧克主動家你的才是正真需要鏟品的聲明我們不是佳人軟...\n",
      "2         1  官方認證急招打字員官方認證急招打字員官方認證急招打字員官方認證急招打字員官方認證急招打字員官...\n",
      "3         1                        誠日入誠日入誠日入誠日入誠日入誠日入誠日入誠日入誠日入\n",
      "4         1      官方認證急招打字員官方認證急招打字員官方認證急招打字員官方認證急招打字員官方認證急招打字員\n",
      "...     ...                                                ...\n",
      "15959     0   我本人申請的有贊帳號為什麼不能進行微信公眾號授權我本人申請的有贊帳號為什麼不能進行微信公眾號授權\n",
      "15960     0                   怎麼綁定不了公眾號呢有圖點我有微信公眾號立即設置進入就變成這樣了\n",
      "15961     0  為什麼訂單已生成卻沒有發貨按鈕選擇我們的蛋糕有贊微商城客人昨天上午拍了個蛋糕我們已在下午送貨...\n",
      "15962     0                        為什麼我註冊後再次登陸打不開頁面電腦上顯示的是這個內容\n",
      "15963     0                          有贊支持商品素材導出嗎有贊支持商品素材導出嗎格式的\n",
      "\n",
      "[15964 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# set_04\n",
    "# https://github.com/youzan/YZSpamFilter/blob/master/spam.txt\n",
    "# https://github.com/youzan/YZSpamFilter/blob/master/ham.txt\n",
    "\n",
    "df4_spam = pd.read_csv('../dataset/set_04/spam.txt', header=None, encoding=UTF_8)\n",
    "df4_spam[1] = 1\n",
    "\n",
    "df4_ham = pd.read_csv('../dataset/set_04/ham.txt', header=None, encoding=UTF_8)\n",
    "df4_ham[1] = 0\n",
    "\n",
    "df4 = pd.concat([df4_spam, df4_ham], ignore_index=True)\n",
    "\n",
    "# define header and reorder columns\n",
    "df4.rename(columns={0: 'content', 1: 'spam'}, inplace=True)\n",
    "df4 = df4.reindex(columns=['spam', 'content'])\n",
    "\n",
    "# transform content to lower case before any further process\n",
    "df4.content = df4.content.str.lower()\n",
    "\n",
    "# transform to tc\n",
    "df4.content = df4.content.map(scToTc)\n",
    "\n",
    "print(df4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       spam                                            content\n",
      "0         1  urgent! call 09061749602 from landline. your c...\n",
      "1         1  +449071512431 urgent! this is the 2nd attempt ...\n",
      "2         1  free for 1st week! no1 nokia tone 4 ur mob eve...\n",
      "3         1  urgent! call 09066612661 from landline. your c...\n",
      "4         1  winner!! as a valued network customer you have...\n",
      "...     ...                                                ...\n",
      "22345     0   我本人申請的有贊帳號為什麼不能進行微信公眾號授權我本人申請的有贊帳號為什麼不能進行微信公眾號授權\n",
      "22346     0                   怎麼綁定不了公眾號呢有圖點我有微信公眾號立即設置進入就變成這樣了\n",
      "22347     0  為什麼訂單已生成卻沒有發貨按鈕選擇我們的蛋糕有贊微商城客人昨天上午拍了個蛋糕我們已在下午送貨...\n",
      "22348     0                        為什麼我註冊後再次登陸打不開頁面電腦上顯示的是這個內容\n",
      "22349     0                          有贊支持商品素材導出嗎有贊支持商品素材導出嗎格式的\n",
      "\n",
      "[22350 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# merge set_01, set_02 and set_04 data\n",
    "df = pd.concat([df1, df2, df4], ignore_index=True)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorry i missed your call let's talk when you have the time. i'm on 07090201529\n",
      "sorry i missed your call let us talk when you have the time. i am on 07090201529\n",
      "i luv u soo much u don?t understand how special u r 2 me ring u 2morrow luv u xxx\n",
      "i love you soo much you do not understand how special you r 2 me ring you 2morrow love you xxx\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "\n",
    "\n",
    "def expandContraction(text):\n",
    "    text = str(text)\n",
    "\n",
    "    # use regex for handling some ' is ?\n",
    "    # ??? if asdfklhli'maksdjhfl\n",
    "\n",
    "    # specific\n",
    "    text = re.sub(r'i[\\'?]m', 'i am', text)\n",
    "    text = re.sub(r'let[\\'?]s', 'let us', text)\n",
    "    text = re.sub(r'don[\\'?]t', 'do not', text)\n",
    "    text = re.sub(r'can[\\'?]t', 'can not', text)\n",
    "    text = re.sub(r'won[\\'?]t', 'will not', text)\n",
    "\n",
    "    # general\n",
    "    text = re.sub(r'[\\'?]s', ' is', text)\n",
    "    text = re.sub(r'[\\'?]re', ' are', text)\n",
    "    text = re.sub(r'[\\'?]ll', ' will', text)\n",
    "    text = re.sub(r'[\\'?]d', ' would', text)\n",
    "    text = re.sub(r'[\\'?]ve', ' have', text)\n",
    "    text = re.sub(r'n[\\'?]t', ' not', text)\n",
    "\n",
    "    # library\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# for testing only\n",
    "print(df.content[24])\n",
    "print(expandContraction(df.content[24]))\n",
    "print(df.content[110])\n",
    "print(expandContraction(df.content[110]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       spam                                            content\n",
      "0         1  urgent call number from landline your complime...\n",
      "1         1  number urgent this is the number nd attempt to...\n",
      "2         1  free for number st week no number nokia tone n...\n",
      "3         1  urgent call number from landline your compleme...\n",
      "4         1  winner as a valued network customer you have b...\n",
      "...     ...                                                ...\n",
      "22345     0   我本人申請的有贊帳號為什麼不能進行微信公眾號授權我本人申請的有贊帳號為什麼不能進行微信公眾號授權\n",
      "22346     0                   怎麼綁定不了公眾號呢有圖點我有微信公眾號立即設置進入就變成這樣了\n",
      "22347     0  為什麼訂單已生成卻沒有發貨按鈕選擇我們的蛋糕有贊微商城客人昨天上午拍了個蛋糕我們已在下午送貨...\n",
      "22348     0                        為什麼我註冊後再次登陸打不開頁面電腦上顯示的是這個內容\n",
      "22349     0                          有贊支持商品素材導出嗎有贊支持商品素材導出嗎格式的\n",
      "\n",
      "[22350 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def cleanData(df):\n",
    "    # expand contraction\n",
    "    df.content = df.content.map(expandContraction)\n",
    "\n",
    "    # replace hyperlink\n",
    "    df.content = df.content.map(lambda row: re.sub(r'http[s]?:\\/\\/[\\w\\/.?=-]+', ' link ', row))\n",
    "\n",
    "    # replace email address\n",
    "    df.content = df.content.map(lambda row: re.sub(r'[\\w\\.+]+@[\\w\\.]+\\.[a-z]{2,}', ' email ', row))\n",
    "\n",
    "    # replace currency sign\n",
    "    df.content = df.content.map(lambda row: re.sub(r'[\\$€£¥]', ' money ', row))\n",
    "\n",
    "    # replace number\n",
    "    df.content = df.content.map(lambda row: re.sub(r'[\\d]+', ' number ', row))\n",
    "\n",
    "    # replace special char, other than a-z, A-Z, 0-9 and chinese\n",
    "    df.content = df.content.map(lambda row: re.sub(r'[^a-zA-Z0-9\\u4E00-\\u9FFF]+', ' ', row))\n",
    "\n",
    "    # replace new line (carriage return and line feed)\n",
    "    df.content = df.content.map(lambda row: re.sub(r'[\\r\\n]', ' ', row))\n",
    "\n",
    "    # replace white space\n",
    "    df.content = df.content.map(lambda row: re.sub(r'[\\s]{2,}', ' ', row))\n",
    "    df.content = df.content.map(lambda row: re.sub(r'^[\\s]+|[\\s]+$', '', row))\n",
    "\n",
    "\n",
    "# clean the data for set_01 and set_02\n",
    "cleanData(df)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def stopWords(df):\n",
    "    words = stopwords.words('english')\n",
    "\n",
    "    df.content = df.content.map(lambda row: ' '.join([word for word in row.split() if word not in (words)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def stemming(df):\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    df.content = df.content.map(lambda row: ' '.join([stemmer.stem(word) for word in row.split()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def lemmatization(df):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    df.content = df.content.map(lambda row: ' '.join([lemmatizer.lemmatize(word) for word in row.split()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path, PurePath\n",
    "\n",
    "\n",
    "def transFileToTc(sc_path, force=False):\n",
    "    path = Path(sc_path)\n",
    "\n",
    "    folder = path.parent.absolute()\n",
    "    stem = path.stem\n",
    "    suffix = path.suffix\n",
    "\n",
    "    tc_path = PurePath(folder, stem + '_tc' + suffix)\n",
    "\n",
    "    if not Path(tc_path).is_file() or force:\n",
    "        sc_file = open(sc_path, 'r', encoding=UTF_8)\n",
    "\n",
    "        tc_content = scToTc(sc_file.read())\n",
    "        tc_content = tc_content.lower()\n",
    "\n",
    "        tc_array = tc_content.split('\\n')\n",
    "        tc_array = list(dict.fromkeys(tc_array))\n",
    "\n",
    "        tc_file = open(tc_path, 'w', encoding=UTF_8)\n",
    "        tc_file.write('\\n'.join(tc_array))\n",
    "\n",
    "    return str(tc_path)\n",
    "\n",
    "\n",
    "# use it later, for chinese text segmentation\n",
    "dict_big_tc = transFileToTc('./jieba/dict_big.txt', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhance dictionary for chinese text segmentation\n",
    "def enhanceDictionary(tc_path, other_list):\n",
    "    if Path(tc_path).is_file():\n",
    "        tc_file = open(tc_path, 'r', encoding=UTF_8)\n",
    "\n",
    "        tc_content = tc_file.read()\n",
    "\n",
    "        tc_key_array = tc_content.split('\\n')\n",
    "        tc_key_array = list(dict.fromkeys(tc_key_array))\n",
    "        tc_key_array = [word.split()[0] for word in tc_key_array if len(word) > 0]\n",
    "\n",
    "        # convert to set for faster comparison\n",
    "        tc_key_set = set(tc_key_array)\n",
    "\n",
    "        for other_path in other_list:\n",
    "            other_file = open(other_path, 'r', encoding=UTF_8)\n",
    "\n",
    "            other_content = scToTc(other_file.read())\n",
    "            other_content = other_content.lower()\n",
    "\n",
    "            other_array = other_content.split('\\n')\n",
    "            other_array = list(dict.fromkeys(other_array))\n",
    "            other_array = [word for word in other_array if len(word) > 0]\n",
    "\n",
    "            new_array = [word for word in other_array if word.split()[0] not in tc_key_set]\n",
    "\n",
    "            for word in new_array:\n",
    "                tc_content += word + '\\n'\n",
    "\n",
    "                tc_key_array.append(word)\n",
    "\n",
    "        tc_file = open(tc_path, 'w', encoding=UTF_8)\n",
    "        tc_file.write(tc_content)\n",
    "\n",
    "\n",
    "# merge two more dictionaries\n",
    "enhanceDictionary(dict_big_tc, ['./jieba/dict_original.txt', './jieba/dict_taiwan.txt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "\n",
    "def segmentation(df):\n",
    "    jieba.load_userdict(dict_big_tc)\n",
    "    # jieba.load_userdict('./jieba/dict_custom.txt')\n",
    "\n",
    "    df.content = df.content.map(lambda row: ' '.join(jieba.cut(row)))\n",
    "    df.content = df.content.map(lambda row: re.sub(r'[\\s]{2,}', ' ', row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_03\n",
    "# https://github.com/Cypher-Z/FBS_SMS_Dataset\n",
    "\n",
    "df3 = pd.DataFrame()\n",
    "\n",
    "set_03_dir = '../dataset/set_03/'\n",
    "set_03_files = Path(set_03_dir).glob('*')\n",
    "\n",
    "for file in set_03_files:\n",
    "    df3_one = pd.read_csv(file, header=None, encoding=UTF_8)\n",
    "    df3_one[1] = 1\n",
    "\n",
    "    # define header and reorder columns\n",
    "    df3_one.rename(columns={0: 'content', 1: 'spam'}, inplace=True)\n",
    "    df3_one = df3_one.reindex(columns=['spam', 'content'])\n",
    "\n",
    "    # since df3 is well processed, special handling for some keywords\n",
    "    df3_one.content = df3_one.content.map(lambda row:  row.replace('URL', ' link '))\n",
    "    df3_one.content = df3_one.content.map(lambda row:  row.replace('HOTLINE', ' number '))\n",
    "    df3_one.content = df3_one.content.map(lambda row:  row.replace('CELLPHONE', ' number '))\n",
    "    df3_one.content = df3_one.content.map(lambda row:  row.replace('PHONE', ' number '))\n",
    "    df3_one.content = df3_one.content.map(lambda row:  row.replace('DIGIT', ' number '))\n",
    "    df3_one.content = df3_one.content.map(lambda row:  row.replace('NAME', ' '))\n",
    "    df3_one.content = df3_one.content.map(lambda row:  row.replace('PLACE', ' '))\n",
    "\n",
    "    # transform content to lower case before any further process\n",
    "    df3_one.content = df3_one.content.str.lower()\n",
    "\n",
    "    # transform to tc\n",
    "    df3_one.content = df3_one.content.map(scToTc)\n",
    "\n",
    "    cleanData(df3_one)\n",
    "\n",
    "    # remove if content no any space\n",
    "    df3_one = df3_one[df3_one.content.str.contains(r'[\\s]+')]\n",
    "\n",
    "    df3 = pd.concat([df3, df3_one.copy()], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\user\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.201 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "actions = 'STOP_WORDS STEMMING LEMMATIZATION'.split()\n",
    "\n",
    "for p in itertools.product([0, 1], repeat=3):\n",
    "    params = dict(zip(actions, p))\n",
    "\n",
    "    STOP_WORDS = params['STOP_WORDS']\n",
    "    STEMMING = params['STEMMING']\n",
    "    LEMMATIZATION = params['LEMMATIZATION']\n",
    "\n",
    "    newDf = df.copy()\n",
    "\n",
    "    if (bool(STOP_WORDS)):\n",
    "        stopWords(newDf)\n",
    "\n",
    "    if (bool(STEMMING)):\n",
    "        stemming(newDf)\n",
    "\n",
    "    if (bool(LEMMATIZATION)):\n",
    "        lemmatization(newDf)\n",
    "\n",
    "    segmentation(newDf)\n",
    "\n",
    "    newDf = pd.concat([newDf, df3], ignore_index=True)\n",
    "    newDf = newDf.dropna()\n",
    "    newDf = newDf.drop_duplicates()\n",
    "\n",
    "    newDf.to_csv(f'../dataset/set_01_02_03_04_{STOP_WORDS}_{STEMMING}_{LEMMATIZATION}_new.csv', header=None, index=False, encoding=UTF_8)\n",
    "\n",
    "    if (STOP_WORDS == 1 and STEMMING == 1 and LEMMATIZATION == 1):\n",
    "        df = newDf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
