{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5886\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read csv and define header\n",
    "df = pd.read_csv('../dataset/set_01/set_01.csv', header=None, encoding='ascii')\n",
    "df.rename(columns={0: 'spam', 1: 'content'}, inplace=True)\n",
    "\n",
    "# transform content to lower case before any further process\n",
    "df.content = df.content.str.lower()\n",
    "\n",
    "print(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorry i missed your call let's talk when you have the time. i'm on 07090201529\n",
      "sorry i missed your call let us talk when you have the time. i am on 07090201529\n",
      "i luv u soo much u don?t understand how special u r 2 me ring u 2morrow luv u xxx\n",
      "i luv u soo much u do not understand how special u r 2 me ring u 2morrow luv u xxx\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def deContact(text):\n",
    "    text = str(text)\n",
    "\n",
    "    # use regex for handling some ' is ?\n",
    "    # ??? if asdfklhli'maksdjhfl\n",
    "\n",
    "    # specific\n",
    "    text = re.sub(r'i[\\'?]m', 'i am', text)\n",
    "    text = re.sub(r'let[\\'?]s', 'let us', text)\n",
    "    text = re.sub(r'don[\\'?]t', 'do not', text)\n",
    "    text = re.sub(r'can[\\'?]t', 'can not', text)\n",
    "    text = re.sub(r'won[\\'?]t', 'will not', text)\n",
    "\n",
    "    # general\n",
    "    text = re.sub(r'[\\'?]s', ' is', text)\n",
    "    text = re.sub(r'[\\'?]re', ' are', text)\n",
    "    text = re.sub(r'[\\'?]ll', ' will', text)\n",
    "    text = re.sub(r'[\\'?]d', ' would', text)\n",
    "    text = re.sub(r'[\\'?]ve', ' have', text)\n",
    "    text = re.sub(r'n[\\'?]t', ' not', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# for showing the result only\n",
    "print(df.content[24])\n",
    "print(deContact(df.content[24]))\n",
    "print(df.content[110])\n",
    "print(deContact(df.content[110]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# const for controlling the level of data cleansing\n",
    "\n",
    "DECONTACT = True\n",
    "\n",
    "REPLACE_HYPERLINK = True\n",
    "REPLACE_EMAIL_ADDRESS = True\n",
    "REPLACE_CURRENCY_SIGN = True\n",
    "REPLACE_NUMBER = True\n",
    "REPLACE_SPECIAL_CHAR = True\n",
    "REPLACE_NEW_LINE = True\n",
    "REPLACE_WHITE_SPACE = True\n",
    "\n",
    "LEMMATIZE = True\n",
    "REMOVE_STOP_WORDS = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DECONTACT:\n",
    "    df.content = df.content.map(deContact)\n",
    "\n",
    "if REPLACE_HYPERLINK:\n",
    "    df.content = df.content.map(\n",
    "        lambda row: re.sub(r'http[s]?:\\/\\/[\\w\\/.?=-]+', ' link ', row))\n",
    "\n",
    "if REPLACE_EMAIL_ADDRESS:\n",
    "    df.content = df.content.map(\n",
    "        lambda row: re.sub(r'[\\w\\.+]+@[\\w\\.]+\\.[a-z]{2,}', ' email ', row))\n",
    "\n",
    "if REPLACE_CURRENCY_SIGN:\n",
    "    df.content = df.content.map(\n",
    "        lambda row: re.sub(r'[\\$€£¥]', ' money ', row))\n",
    "\n",
    "if REPLACE_NUMBER:\n",
    "    df.content = df.content.map(\n",
    "        lambda row: re.sub(r'[\\d]+', ' number ', row))\n",
    "\n",
    "if REPLACE_SPECIAL_CHAR:\n",
    "    df.content = df.content.map(\n",
    "        lambda row: re.sub(r'[^a-zA-Z0-9]+', ' ', row))\n",
    "\n",
    "if REPLACE_NEW_LINE:\n",
    "    df.content = df.content.map(\n",
    "        lambda row: re.sub(r'[\\r\\n]', ' ', row))\n",
    "\n",
    "if REPLACE_WHITE_SPACE:\n",
    "    df.content = df.content.map(\n",
    "        lambda row: re.sub(r'[\\s]{2,}', ' ', row))\n",
    "    df.content = df.content.map(\n",
    "        lambda row: re.sub(r'^[\\s]+|[\\s]+$', '', row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "df.content = df.content.map(\n",
    "    lambda row: ' '.join([word for word in row.split() if word not in (stopwords)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df.content = df.content.map(\n",
    "    lambda row: ' '.join([lemmatizer.lemmatize(word) for word in row.split()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../dataset/set_01/set_01_new.csv',\n",
    "          header=None, index=False, encoding='ascii')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      spam                                            content\n",
      "0        1  urgent call number landline complimentary numb...\n",
      "1        1  number urgent number nd attempt contact u u nu...\n",
      "2        1  free number st week number nokia tone number u...\n",
      "3        1  urgent call number landline complementary numb...\n",
      "4        1  winner valued network customer selected receiv...\n",
      "...    ...                                                ...\n",
      "5801     0                      lol grin babe thanks thinking\n",
      "5802     0                       man bus slow think gonna get\n",
      "5803     0  hope text meet smiling let text give reason sm...\n",
      "5804     0  case wake wondering forgot take care something...\n",
      "5852     0            hey gal u wanna meet number dinner n te\n",
      "\n",
      "[5072 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# drop rows can't be used\n",
    "\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = None\n",
    "X_test = None\n",
    "y_train = None\n",
    "y_test = None\n",
    "\n",
    "\n",
    "def useVectorizer(vectorizer):\n",
    "    global X_train, X_test, y_train, y_test\n",
    "\n",
    "    X = vectorizer.fit_transform(df.content).toarray()\n",
    "    y = df.spam\n",
    "\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.20, random_state=0)\n",
    "\n",
    "    print(X_train)\n",
    "    print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def useClassifier(classifier):\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(accuracy)\n",
    "\n",
    "\n",
    "def useAllClassifier():\n",
    "    useClassifier(AdaBoostClassifier(n_estimators=100, random_state=0))\n",
    "    useClassifier(DecisionTreeClassifier(random_state=0))\n",
    "    useClassifier(DecisionTreeRegressor(random_state=0))\n",
    "    useClassifier(GaussianNB())\n",
    "    useClassifier(GradientBoostingClassifier(n_estimators=100,\n",
    "                  learning_rate=1.0, max_depth=1, random_state=0))\n",
    "    useClassifier(KMeans(n_clusters=2, random_state=0))\n",
    "    useClassifier(KNeighborsClassifier(n_neighbors=3))\n",
    "    useClassifier(LogisticRegression(random_state=0))\n",
    "    useClassifier(MultinomialNB())\n",
    "    useClassifier(RandomForestClassifier(max_depth=2, random_state=0))\n",
    "    useClassifier(SGDClassifier(max_iter=1000, tol=1e-3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5072, 7084)\n",
      "(5072,)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "(4057, 7084)\n",
      "0.9773399014778326\n",
      "0.9665024630541872\n",
      "0.9665024630541872\n",
      "0.8866995073891626\n",
      "0.9556650246305419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9527093596059113\n",
      "0.9556650246305419\n",
      "0.9724137931034482\n",
      "0.961576354679803\n",
      "0.8955665024630541\n",
      "0.9753694581280788\n",
      "\n",
      "(5072, 7084)\n",
      "(5072,)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(4057, 7084)\n",
      "0.9822660098522168\n",
      "0.961576354679803\n",
      "0.961576354679803\n",
      "0.8827586206896552\n",
      "0.9507389162561576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05123152709359606\n",
      "0.941871921182266\n",
      "0.9655172413793104\n",
      "0.9665024630541872\n",
      "0.8955665024630541\n",
      "0.9793103448275862\n",
      "\n",
      "(5072, 16)\n",
      "(5072,)\n",
      "[[ 0.         -0.30151134  0.30151134 ...  0.30151134  0.\n",
      "   0.30151134]\n",
      " [ 0.          0.         -0.42857143 ...  0.14285714  0.14285714\n",
      "   0.        ]\n",
      " [ 0.          0.          0.70710678 ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.         -0.70710678 ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "  -0.5       ]\n",
      " [ 0.          0.81649658  0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "(4057, 16)\n",
      "0.9349753694581281\n",
      "0.9192118226600985\n",
      "0.9192118226600985\n",
      "0.9211822660098522\n",
      "0.9182266009852217\n",
      "0.21970443349753693\n",
      "0.9349753694581281\n",
      "0.941871921182266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to MultinomialNB (input X)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m()\n\u001b[0;32m     15\u001b[0m useVectorizer(HashingVectorizer(n_features\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m4\u001b[39m))\n\u001b[1;32m---> 16\u001b[0m useAllClassifier()\n",
      "Cell \u001b[1;32mIn[10], line 34\u001b[0m, in \u001b[0;36museAllClassifier\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m useClassifier(KNeighborsClassifier(n_neighbors\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m))\n\u001b[0;32m     33\u001b[0m useClassifier(LogisticRegression(random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[1;32m---> 34\u001b[0m useClassifier(MultinomialNB())\n\u001b[0;32m     35\u001b[0m useClassifier(RandomForestClassifier(max_depth\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[0;32m     36\u001b[0m useClassifier(SGDClassifier(max_iter\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, tol\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m))\n",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m, in \u001b[0;36museClassifier\u001b[1;34m(classifier)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39museClassifier\u001b[39m(classifier):\n\u001b[1;32m---> 16\u001b[0m     classifier\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     18\u001b[0m     y_pred \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m     19\u001b[0m     accuracy \u001b[39m=\u001b[39m accuracy_score(y_test, y_pred)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\naive_bayes.py:776\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    774\u001b[0m n_classes \u001b[39m=\u001b[39m Y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m    775\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_counters(n_classes, n_features)\n\u001b[1;32m--> 776\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count(X, Y)\n\u001b[0;32m    777\u001b[0m alpha \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_alpha()\n\u001b[0;32m    778\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_feature_log_prob(alpha)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\naive_bayes.py:898\u001b[0m, in \u001b[0;36mMultinomialNB._count\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    896\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_count\u001b[39m(\u001b[39mself\u001b[39m, X, Y):\n\u001b[0;32m    897\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 898\u001b[0m     check_non_negative(X, \u001b[39m\"\u001b[39;49m\u001b[39mMultinomialNB (input X)\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    899\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_count_ \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m safe_sparse_dot(Y\u001b[39m.\u001b[39mT, X)\n\u001b[0;32m    900\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_count_ \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m Y\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\validation.py:1418\u001b[0m, in \u001b[0;36mcheck_non_negative\u001b[1;34m(X, whom)\u001b[0m\n\u001b[0;32m   1415\u001b[0m     X_min \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mmin(X)\n\u001b[0;32m   1417\u001b[0m \u001b[39mif\u001b[39;00m X_min \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 1418\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNegative values in data passed to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m whom)\n",
      "\u001b[1;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "useVectorizer(CountVectorizer())\n",
    "useAllClassifier()\n",
    "\n",
    "print()\n",
    "\n",
    "useVectorizer(TfidfVectorizer())\n",
    "useAllClassifier()\n",
    "\n",
    "print()\n",
    "\n",
    "useVectorizer(HashingVectorizer(n_features=2**4))\n",
    "useAllClassifier()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
