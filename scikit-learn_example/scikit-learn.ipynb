{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "UTF_8 = 'utf-8'\n",
    "\n",
    "# read csv, no header\n",
    "df1 = pd.read_csv('../dataset/set_01/set_01.csv',\n",
    "                  header=None, encoding=UTF_8)\n",
    "\n",
    "# define header\n",
    "df1.rename(columns={0: 'spam', 1: 'content'}, inplace=True)\n",
    "\n",
    "# transform content to lower case before any further process\n",
    "df1.content = df1.content.str.lower()\n",
    "\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv, get two columns\n",
    "df2 = pd.read_csv('../dataset/set_02/SMS collect form (Responses) - Form Responses 1.csv',\n",
    "                  usecols=['Content', 'Spam or Ham'], encoding=UTF_8)\n",
    "\n",
    "# rename and reorder columns\n",
    "df2 = df2.rename(columns={'Content': 'content', 'Spam or Ham': 'spam'})\n",
    "df2 = df2.reindex(columns=['spam', 'content'])\n",
    "\n",
    "# Spam = 1, Ham = 0\n",
    "df2.spam = df2.spam.map({'Spam': 1, 'Ham': 0})\n",
    "\n",
    "# transform content to lower case before any further process\n",
    "df2.content = df2.content.str.lower()\n",
    "\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from zhconv import convert\n",
    "\n",
    "\n",
    "def scToTc(text):\n",
    "    text = convert(text, 'zh-tw')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# for testing only\n",
    "print(re.sub(r'[\\r\\n]', ' ', df2.content[26]))\n",
    "print(re.sub(r'[\\r\\n]', ' ', scToTc(df2.content[26])))\n",
    "print(re.sub(r'[\\r\\n]', ' ', df2.content[146]))\n",
    "print(re.sub(r'[\\r\\n]', ' ', scToTc(df2.content[146])))\n",
    "\n",
    "df2.content = df2.content.map(scToTc)\n",
    "\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge two datasets\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expandContract(text):\n",
    "    text = str(text)\n",
    "\n",
    "    # use regex for handling some ' is ?\n",
    "    # ??? if asdfklhli'maksdjhfl\n",
    "\n",
    "    # specific\n",
    "    text = re.sub(r'i[\\'?]m', 'i am', text)\n",
    "    text = re.sub(r'let[\\'?]s', 'let us', text)\n",
    "    text = re.sub(r'don[\\'?]t', 'do not', text)\n",
    "    text = re.sub(r'can[\\'?]t', 'can not', text)\n",
    "    text = re.sub(r'won[\\'?]t', 'will not', text)\n",
    "\n",
    "    # general\n",
    "    text = re.sub(r'[\\'?]s', ' is', text)\n",
    "    text = re.sub(r'[\\'?]re', ' are', text)\n",
    "    text = re.sub(r'[\\'?]ll', ' will', text)\n",
    "    text = re.sub(r'[\\'?]d', ' would', text)\n",
    "    text = re.sub(r'[\\'?]ve', ' have', text)\n",
    "    text = re.sub(r'n[\\'?]t', ' not', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# for testing only\n",
    "print(df.content[24])\n",
    "print(expandContract(df.content[24]))\n",
    "print(df.content[110])\n",
    "print(expandContract(df.content[110]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# const for controlling the level of data cleansing\n",
    "EXPAND_CONTRACT = True\n",
    "\n",
    "REPLACE_HYPERLINK = True\n",
    "REPLACE_EMAIL_ADDRESS = True\n",
    "REPLACE_CURRENCY_SIGN = True\n",
    "REPLACE_NUMBER = True\n",
    "REPLACE_SPECIAL_CHAR = True\n",
    "REPLACE_NEW_LINE = True\n",
    "REPLACE_WHITE_SPACE = True\n",
    "\n",
    "REMOVE_STOP_WORDS = True\n",
    "STEM = True\n",
    "LEMMATIZE = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataCleansing(df, replaceWhiteSpace=True):\n",
    "    if EXPAND_CONTRACT:\n",
    "        df.content = df.content.map(expandContract)\n",
    "\n",
    "    if REPLACE_HYPERLINK:\n",
    "        df.content = df.content.map(\n",
    "            lambda row: re.sub(r'http[s]?:\\/\\/[\\w\\/.?=-]+', ' link ', row))\n",
    "\n",
    "    if REPLACE_EMAIL_ADDRESS:\n",
    "        df.content = df.content.map(\n",
    "            lambda row: re.sub(r'[\\w\\.+]+@[\\w\\.]+\\.[a-z]{2,}', ' email ', row))\n",
    "\n",
    "    if REPLACE_CURRENCY_SIGN:\n",
    "        df.content = df.content.map(\n",
    "            lambda row: re.sub(r'[\\$€£¥]', ' money ', row))\n",
    "\n",
    "    if REPLACE_NUMBER:\n",
    "        df.content = df.content.map(\n",
    "            lambda row: re.sub(r'[\\d]+', ' number ', row))\n",
    "\n",
    "    if REPLACE_SPECIAL_CHAR:\n",
    "        df.content = df.content.map(lambda row: re.sub(\n",
    "            r'[^a-zA-Z0-9\\u4E00-\\u9FFF]+', ' ', row))\n",
    "\n",
    "    if REPLACE_NEW_LINE:\n",
    "        df.content = df.content.map(lambda row: re.sub(r'[\\r\\n]', ' ', row))\n",
    "\n",
    "    if REPLACE_WHITE_SPACE and replaceWhiteSpace:\n",
    "        df.content = df.content.map(lambda row: re.sub(r'[\\s]{2,}', ' ', row))\n",
    "        df.content = df.content.map(\n",
    "            lambda row: re.sub(r'^[\\s]+|[\\s]+$', '', row))\n",
    "\n",
    "\n",
    "dataCleansing(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "if REMOVE_STOP_WORDS:\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    stopwords = stopwords.words('english')\n",
    "\n",
    "    df.content = df.content.map(\n",
    "        lambda row: ' '.join([word for word in row.split() if word not in (stopwords)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if STEM:\n",
    "    from nltk.stem import PorterStemmer\n",
    "\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    df.content = df.content.map(\n",
    "        lambda row: ' '.join([stemmer.stem(word) for word in row.split()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LEMMATIZE:\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    df.content = df.content.map(\n",
    "        lambda row: ' '.join([lemmatizer.lemmatize(word) for word in row.split()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path, PurePath\n",
    "\n",
    "\n",
    "def transFileToTc(sc_path, force=False):\n",
    "    path = Path(sc_path)\n",
    "\n",
    "    folder = path.parent.absolute()\n",
    "    stem = path.stem\n",
    "    suffix = path.suffix\n",
    "\n",
    "    tc_path = PurePath(folder, stem + '_tc' + suffix)\n",
    "\n",
    "    if not Path(tc_path).is_file() or force:\n",
    "        sc_file = open(sc_path, 'r', encoding=UTF_8)\n",
    "\n",
    "        tc_content = scToTc(sc_file.read())\n",
    "        tc_content = tc_content.lower()\n",
    "\n",
    "        tc_array = tc_content.split('\\n')\n",
    "        tc_array = list(dict.fromkeys(tc_array))\n",
    "\n",
    "        tc_file = open(tc_path, 'w', encoding=UTF_8)\n",
    "        tc_file.write('\\n'.join(tc_array))\n",
    "\n",
    "    return str(tc_path)\n",
    "\n",
    "\n",
    "dict_big_tc = transFileToTc('./jieba/dict_big.txt', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "jieba.load_userdict(dict_big_tc)\n",
    "# jieba.load_userdict('./jieba/dict_custom.txt')\n",
    "\n",
    "df.content = df.content.map(lambda row: ' '.join(jieba.cut(row)))\n",
    "df.content = df.content.map(lambda row: re.sub(r'[\\s]{2,}', ' ', row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_03_dir = '../dataset/set_03/'\n",
    "set_03_files = Path(set_03_dir).glob('*')\n",
    "\n",
    "for file in set_03_files:\n",
    "    df3 = pd.read_csv(file, header=None, encoding=UTF_8)\n",
    "    df3[1] = 1\n",
    "\n",
    "    # define header\n",
    "    df3.rename(columns={0: 'content', 1: 'spam'}, inplace=True)\n",
    "    # reorder columns\n",
    "    df3 = df3.reindex(columns=['spam', 'content'])\n",
    "\n",
    "    # since df3 is well processed, special handling for some keywords\n",
    "    df3.content = df3.content.map(lambda row:  row.replace('URL', ' link '))\n",
    "    df3.content = df3.content.map(\n",
    "        lambda row:  row.replace('HOTLINE', ' number '))\n",
    "    df3.content = df3.content.map(\n",
    "        lambda row:  row.replace('CELLPHONE', ' number '))\n",
    "    df3.content = df3.content.map(\n",
    "        lambda row:  row.replace('PHONE', ' number '))\n",
    "    df3.content = df3.content.map(\n",
    "        lambda row:  row.replace('DIGIT', ' number '))\n",
    "    df3.content = df3.content.map(lambda row:  row.replace('NAME', ' '))\n",
    "    df3.content = df3.content.map(lambda row:  row.replace('PLACE', ' '))\n",
    "\n",
    "    # transform content to lower case before any further process\n",
    "    df3.content = df3.content.str.lower()\n",
    "\n",
    "    # transform to tc\n",
    "    df3.content = df3.content.map(scToTc)\n",
    "\n",
    "    dataCleansing(df3)\n",
    "\n",
    "    # remove if content no any space\n",
    "    df3 = df3[df3.content.str.contains(r'[\\s]+')]\n",
    "\n",
    "    df = pd.concat([df, df3], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows can't be used\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../dataset/set_01_02_03_new.csv',\n",
    "          header=None, index=False, encoding=UTF_8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = None\n",
    "X_test = None\n",
    "y_train = None\n",
    "y_test = None\n",
    "\n",
    "\n",
    "def useVectorizer(vectorizer):\n",
    "    global X_train, X_test, y_train, y_test\n",
    "\n",
    "    X = vectorizer.fit_transform(df.content).toarray()\n",
    "    y = df.spam\n",
    "\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.20, random_state=0)\n",
    "\n",
    "    print(X_train)\n",
    "    print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def useClassifier(classifier):\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(accuracy)\n",
    "\n",
    "\n",
    "def useAllClassifier():\n",
    "    useClassifier(AdaBoostClassifier(n_estimators=100, random_state=0))\n",
    "    useClassifier(DecisionTreeClassifier(random_state=0))\n",
    "    useClassifier(DecisionTreeRegressor(random_state=0))\n",
    "    useClassifier(GaussianNB())\n",
    "    useClassifier(GradientBoostingClassifier(n_estimators=100,\n",
    "                  learning_rate=1.0, max_depth=1, random_state=0))\n",
    "    useClassifier(KMeans(n_clusters=2, random_state=0))\n",
    "    useClassifier(KNeighborsClassifier(n_neighbors=3))\n",
    "    useClassifier(LogisticRegression(random_state=0))\n",
    "    useClassifier(MultinomialNB())\n",
    "    useClassifier(RandomForestClassifier(max_depth=2, random_state=0))\n",
    "    useClassifier(SGDClassifier(max_iter=1000, tol=1e-3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "useVectorizer(CountVectorizer())\n",
    "useAllClassifier()\n",
    "\n",
    "# not run, will error\n",
    "if False:\n",
    "    print()\n",
    "\n",
    "    useVectorizer(TfidfVectorizer())\n",
    "    useAllClassifier()\n",
    "\n",
    "    print()\n",
    "\n",
    "    useVectorizer(HashingVectorizer(n_features=2**4))\n",
    "    useAllClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
