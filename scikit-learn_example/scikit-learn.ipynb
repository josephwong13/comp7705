{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      spam                                            content\n",
      "0        1  urgent! call 09061749602 from landline. your c...\n",
      "1        1  +449071512431 urgent! this is the 2nd attempt ...\n",
      "2        1  free for 1st week! no1 nokia tone 4 ur mob eve...\n",
      "3        1  urgent! call 09066612661 from landline. your c...\n",
      "4        1  winner!! as a valued network customer you have...\n",
      "...    ...                                                ...\n",
      "5881     0                ok lor ?_ reaching then message me.\n",
      "5882     1  marvel mobile play the official ultimate spide...\n",
      "5883     0             it???s reassuring in this crazy world.\n",
      "5884     1  asked 3mobile if 0870 chatlines inclu in free ...\n",
      "5885     0              will ?_ b going to esplanade fr home?\n",
      "\n",
      "[5886 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "UTF_8 = 'utf-8'\n",
    "\n",
    "# read csv, no header\n",
    "df1 = pd.read_csv('../dataset/set_01/set_01.csv',\n",
    "                  header=None, encoding=UTF_8)\n",
    "\n",
    "# define header\n",
    "df1.rename(columns={0: 'spam', 1: 'content'}, inplace=True)\n",
    "\n",
    "# transform content to lower case before any further process\n",
    "df1.content = df1.content.str.lower()\n",
    "\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     spam                                            content\n",
      "0       1    [netflix] : 無法處理你的自動付款。你的帳戶將被禁用。t.co/ntakhpfwqr\n",
      "1       1  您好！我系橙橙\\n邀請您加细妹微信：76169639 睇朋友圈保有您喜歡嘅哦！\\n有國際大牌...\n",
      "2       1  您好！{本店支持淘宝店铺下单，淘宝店铺付款}\\n請加我微信（wechat）: 1198632...\n",
      "3       0  aeon: 即日起至3月5日，登入「aeon 香港」手機應用程式或aeon網上客戶服務，申請...\n",
      "4       1  恒生hang seng：信用卡「現金分期」計劃：\\n已為你預先批核多一筆現金，你的稅季限定個...\n",
      "..    ...                                                ...\n",
      "143     0  渣打香港:您的渣打信用卡結尾7645於03/27 在 vennic limit 有一項hkd...\n",
      "144     0  nti have received your order for\\n1 x cwp - ca...\n",
      "145     0         your nti mall verification code is: 467255\n",
      "146     0    【阿里巴巴】验证码798505，您正在登录验证，切勿将验证码泄露于他人，验证码15分钟内有效。\n",
      "147     0         your requested authentication code: 281577\n",
      "\n",
      "[148 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# read csv, get two columns\n",
    "df2 = pd.read_csv('../dataset/set_02/SMS collect form (Responses) - Form Responses 1.csv',\n",
    "                  usecols=['Content', 'Spam or Ham'], encoding=UTF_8)\n",
    "\n",
    "# rename and reorder columns\n",
    "df2 = df2.rename(columns={'Content': 'content', 'Spam or Ham': 'spam'})\n",
    "df2 = df2.reindex(columns=['spam', 'content'])\n",
    "\n",
    "# Spam = 1, Ham = 0\n",
    "df2.spam = df2.spam.map({'Spam': 1, 'Ham': 0})\n",
    "\n",
    "# transform content to lower case before any further process\n",
    "df2.content = df2.content.str.lower()\n",
    "\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "亲爱的chankwan pok先生或女士，欢迎您加入“心享”计划 hpr”，您的会员编码为：111023621183，初始密码为：006653。恭喜您可尊享通过“中旅酒店心享会”公众号推荐朋友加入会员活动，被推荐的新会员还可以领取代金券礼包！详询400-669-0000（大陆地区）、852-36040000（港澳地区）。\n",
      "親愛的chankwan pok先生或女士，歡迎您加入「心享」計劃 hpr」，您的會員編碼為：111023621183，初始密碼為：006653。恭喜您可尊享通過「中旅酒店心享會」公眾號推薦朋友加入會員活動，被推薦的新會員還可以領取代金券禮包！詳詢400-669-0000（大陸地區）、852-36040000（港澳地區）。\n",
      "【阿里巴巴】验证码798505，您正在登录验证，切勿将验证码泄露于他人，验证码15分钟内有效。\n",
      "【阿里巴巴】驗證碼798505，您正在登錄驗證，切勿將驗證碼洩露於他人，驗證碼15分鐘內有效。\n",
      "     spam                                            content\n",
      "0       1    [netflix] : 無法處理你的自動付款。你的帳戶將被禁用。t.co/ntakhpfwqr\n",
      "1       1  您好！我系橙橙\\n邀請您加細妹微信：76169639 睇朋友圈保有您喜歡嘅哦！\\n有國際大牌...\n",
      "2       1  您好！{本店支持淘寶店鋪下單，淘寶店鋪付款}\\n請加我微信（wechat）: 1198632...\n",
      "3       0  aeon: 即日起至3月5日，登入「aeon 香港」手機應用程式或aeon網上客戶服務，申請...\n",
      "4       1  恒生hang seng：信用卡「現金分期」計劃：\\n已為你預先批核多一筆現金，你的稅季限定個...\n",
      "..    ...                                                ...\n",
      "143     0  渣打香港:您的渣打信用卡結尾7645於03/27 在 vennic limit 有一項hkd...\n",
      "144     0  nti have received your order for\\n1 x cwp - ca...\n",
      "145     0         your nti mall verification code is: 467255\n",
      "146     0    【阿里巴巴】驗證碼798505，您正在登錄驗證，切勿將驗證碼洩露於他人，驗證碼15分鐘內有效。\n",
      "147     0         your requested authentication code: 281577\n",
      "\n",
      "[148 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from zhconv import convert\n",
    "\n",
    "\n",
    "def scToTc(text):\n",
    "    text = convert(text, 'zh-tw')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# for testing only\n",
    "print(re.sub(r'[\\r\\n]', ' ', df2.content[26]))\n",
    "print(re.sub(r'[\\r\\n]', ' ', scToTc(df2.content[26])))\n",
    "print(re.sub(r'[\\r\\n]', ' ', df2.content[146]))\n",
    "print(re.sub(r'[\\r\\n]', ' ', scToTc(df2.content[146])))\n",
    "\n",
    "df2.content = df2.content.map(scToTc)\n",
    "\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      spam                                            content\n",
      "0        1  urgent! call 09061749602 from landline. your c...\n",
      "1        1  +449071512431 urgent! this is the 2nd attempt ...\n",
      "2        1  free for 1st week! no1 nokia tone 4 ur mob eve...\n",
      "3        1  urgent! call 09066612661 from landline. your c...\n",
      "4        1  winner!! as a valued network customer you have...\n",
      "...    ...                                                ...\n",
      "6029     0  渣打香港:您的渣打信用卡結尾7645於03/27 在 vennic limit 有一項hkd...\n",
      "6030     0  nti have received your order for\\n1 x cwp - ca...\n",
      "6031     0         your nti mall verification code is: 467255\n",
      "6032     0    【阿里巴巴】驗證碼798505，您正在登錄驗證，切勿將驗證碼洩露於他人，驗證碼15分鐘內有效。\n",
      "6033     0         your requested authentication code: 281577\n",
      "\n",
      "[6034 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# merge two datasets\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorry i missed your call let's talk when you have the time. i'm on 07090201529\n",
      "sorry i missed your call let us talk when you have the time. i am on 07090201529\n",
      "i luv u soo much u don?t understand how special u r 2 me ring u 2morrow luv u xxx\n",
      "i luv u soo much u do not understand how special u r 2 me ring u 2morrow luv u xxx\n"
     ]
    }
   ],
   "source": [
    "def deContact(text):\n",
    "    text = str(text)\n",
    "\n",
    "    # use regex for handling some ' is ?\n",
    "    # ??? if asdfklhli'maksdjhfl\n",
    "\n",
    "    # specific\n",
    "    text = re.sub(r'i[\\'?]m', 'i am', text)\n",
    "    text = re.sub(r'let[\\'?]s', 'let us', text)\n",
    "    text = re.sub(r'don[\\'?]t', 'do not', text)\n",
    "    text = re.sub(r'can[\\'?]t', 'can not', text)\n",
    "    text = re.sub(r'won[\\'?]t', 'will not', text)\n",
    "\n",
    "    # general\n",
    "    text = re.sub(r'[\\'?]s', ' is', text)\n",
    "    text = re.sub(r'[\\'?]re', ' are', text)\n",
    "    text = re.sub(r'[\\'?]ll', ' will', text)\n",
    "    text = re.sub(r'[\\'?]d', ' would', text)\n",
    "    text = re.sub(r'[\\'?]ve', ' have', text)\n",
    "    text = re.sub(r'n[\\'?]t', ' not', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# for testing only\n",
    "print(df.content[24])\n",
    "print(deContact(df.content[24]))\n",
    "print(df.content[110])\n",
    "print(deContact(df.content[110]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# const for controlling the level of data cleansing\n",
    "DECONTACT = True\n",
    "\n",
    "REPLACE_HYPERLINK = True\n",
    "REPLACE_EMAIL_ADDRESS = True\n",
    "REPLACE_CURRENCY_SIGN = True\n",
    "REPLACE_NUMBER = True\n",
    "REPLACE_SPECIAL_CHAR = True\n",
    "REPLACE_NEW_LINE = True\n",
    "REPLACE_WHITE_SPACE = True\n",
    "\n",
    "LEMMATIZE = True\n",
    "REMOVE_STOP_WORDS = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DECONTACT:\n",
    "    df.content = df.content.map(deContact)\n",
    "\n",
    "if REPLACE_HYPERLINK:\n",
    "    df.content = df.content.map(\n",
    "        lambda row: re.sub(r'http[s]?:\\/\\/[\\w\\/.?=-]+', ' link ', row))\n",
    "\n",
    "if REPLACE_EMAIL_ADDRESS:\n",
    "    df.content = df.content.map(\n",
    "        lambda row: re.sub(r'[\\w\\.+]+@[\\w\\.]+\\.[a-z]{2,}', ' email ', row))\n",
    "\n",
    "if REPLACE_CURRENCY_SIGN:\n",
    "    df.content = df.content.map(lambda row: re.sub(r'[\\$€£¥]', ' money ', row))\n",
    "\n",
    "if REPLACE_NUMBER:\n",
    "    df.content = df.content.map(lambda row: re.sub(r'[\\d]+', ' number ', row))\n",
    "\n",
    "if REPLACE_SPECIAL_CHAR:\n",
    "    df.content = df.content.map(lambda row: re.sub(\n",
    "        r'[^a-zA-Z0-9\\u4E00-\\u9FFF]+', ' ', row))\n",
    "\n",
    "if REPLACE_NEW_LINE:\n",
    "    df.content = df.content.map(lambda row: re.sub(r'[\\r\\n]', ' ', row))\n",
    "\n",
    "if REPLACE_WHITE_SPACE:\n",
    "    df.content = df.content.map(lambda row: re.sub(r'[\\s]{2,}', ' ', row))\n",
    "    df.content = df.content.map(lambda row: re.sub(r'^[\\s]+|[\\s]+$', '', row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "df.content = df.content.map(\n",
    "    lambda row: ' '.join([word for word in row.split() if word not in (stopwords)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df.content = df.content.map(\n",
    "    lambda row: ' '.join([lemmatizer.lemmatize(word) for word in row.split()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path, PurePath\n",
    "\n",
    "\n",
    "def transFileToTc(sc_path, force=False):\n",
    "    path = Path(sc_path)\n",
    "\n",
    "    folder = path.parent.absolute()\n",
    "    stem = path.stem\n",
    "    suffix = path.suffix\n",
    "\n",
    "    tc_path = PurePath(folder, stem + \"_tc\" + suffix)\n",
    "\n",
    "    if not Path(tc_path).is_file() or force:\n",
    "        sc_file = open(sc_path, 'r', encoding=UTF_8)\n",
    "\n",
    "        tc_content = scToTc(sc_file.read())\n",
    "        tc_content = tc_content.lower()\n",
    "\n",
    "        tc_array = tc_content.split('\\n')\n",
    "        tc_array = list(dict.fromkeys(tc_array))\n",
    "\n",
    "        tc_file = open(tc_path, 'w', encoding=UTF_8)\n",
    "        tc_file.write(\"\\n\".join(tc_array))\n",
    "\n",
    "    return str(tc_path)\n",
    "\n",
    "\n",
    "dict_big_tc = transFileToTc('./jieba/dict_big.txt', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\user\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.965 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "jieba.load_userdict(dict_big_tc)\n",
    "# jieba.load_userdict('./jieba/dict_custom.txt')\n",
    "\n",
    "df.content = df.content.map(lambda row: ' '.join(jieba.cut(row)))\n",
    "df.content = df.content.map(lambda row: re.sub(r'[\\s]{2,}', ' ', row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      spam                                            content\n",
      "0        1  urgent call number landline complimentary numb...\n",
      "1        1  number urgent number nd attempt contact u u nu...\n",
      "2        1  free number st week number nokia tone number u...\n",
      "3        1  urgent call number landline complementary numb...\n",
      "4        1  winner valued network customer selected receiv...\n",
      "...    ...                                                ...\n",
      "6028     0  渣打 香港 您 的 渣打 信用卡 結尾 number 於 number number 在 m...\n",
      "6029     0  渣打 香港 您 的 渣打 信用卡 結尾 number 於 number number 在 v...\n",
      "6030     0  nti received order number x cwp car wash packa...\n",
      "6031     0                  nti mall verification code number\n",
      "6033     0               requested authentication code number\n",
      "\n",
      "[5194 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# drop rows can't be used\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../dataset/set_01_02_new.csv',\n",
    "          header=None, index=False, encoding=UTF_8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = None\n",
    "X_test = None\n",
    "y_train = None\n",
    "y_test = None\n",
    "\n",
    "\n",
    "def useVectorizer(vectorizer):\n",
    "    global X_train, X_test, y_train, y_test\n",
    "\n",
    "    X = vectorizer.fit_transform(df.content).toarray()\n",
    "    y = df.spam\n",
    "\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.20, random_state=0)\n",
    "\n",
    "    print(X_train)\n",
    "    print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def useClassifier(classifier):\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(accuracy)\n",
    "\n",
    "\n",
    "def useAllClassifier():\n",
    "    useClassifier(AdaBoostClassifier(n_estimators=100, random_state=0))\n",
    "    useClassifier(DecisionTreeClassifier(random_state=0))\n",
    "    useClassifier(DecisionTreeRegressor(random_state=0))\n",
    "    useClassifier(GaussianNB())\n",
    "    useClassifier(GradientBoostingClassifier(n_estimators=100,\n",
    "                  learning_rate=1.0, max_depth=1, random_state=0))\n",
    "    useClassifier(KMeans(n_clusters=2, random_state=0))\n",
    "    useClassifier(KNeighborsClassifier(n_neighbors=3))\n",
    "    useClassifier(LogisticRegression(random_state=0))\n",
    "    useClassifier(MultinomialNB())\n",
    "    useClassifier(RandomForestClassifier(max_depth=2, random_state=0))\n",
    "    useClassifier(SGDClassifier(max_iter=1000, tol=1e-3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5194, 8545)\n",
      "(5194,)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "(4155, 8545)\n",
      "0.9643888354186718\n",
      "0.9624639076034649\n",
      "0.9624639076034649\n",
      "0.8960538979788258\n",
      "0.9499518768046198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\default\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9278152069297402\n",
      "0.9432146294513956\n",
      "0.971126082771896\n",
      "0.9884504331087585\n",
      "0.8671799807507219\n",
      "0.9788257940327237\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "useVectorizer(CountVectorizer())\n",
    "useAllClassifier()\n",
    "\n",
    "# not run, will error\n",
    "if False:\n",
    "    print()\n",
    "\n",
    "    useVectorizer(TfidfVectorizer())\n",
    "    useAllClassifier()\n",
    "\n",
    "    print()\n",
    "\n",
    "    useVectorizer(HashingVectorizer(n_features=2**4))\n",
    "    useAllClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9884504331087585\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
