{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Embedding, GlobalAveragePooling1D, Bidirectional, LSTM, Flatten, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam, RMSprop, Adamax, SGD, Nadam\n",
    "from keras.regularizers import L1, L2, L1L2\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import gc\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of dataset\n",
    "dir = '../dataset/'\n",
    "datasets = [file for file in Path(dir).glob('*.csv') if not file.name == \"best_dataset.csv\"]\n",
    "\n",
    "datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load data\n",
    "def load_data(filename):\n",
    "    print(filename)\n",
    "\n",
    "    df = pd.read_csv(filename, header=None, encoding='utf-8').dropna()\n",
    "    df.columns = ['label', 'data']\n",
    "\n",
    "    global X, y\n",
    "\n",
    "    X = df['data']\n",
    "    y = df['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create train val test split\n",
    "def split_dataset(X, y):\n",
    "    global X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "    # train 7 : val 2 : test 1\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=7)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.33, random_state=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract feature\n",
    "def extract_feature(vectorizer, X):\n",
    "    # vectorizer.fit(X)\n",
    "\n",
    "    global X_train, X_val, X_test\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X)\n",
    "\n",
    "    X_train = tokenizer.texts_to_sequences(X_train)\n",
    "    X_train = pad_sequences(X_train, maxlen=100, padding='post', truncating='post')\n",
    "\n",
    "    X_val = tokenizer.texts_to_sequences(X_val)\n",
    "    X_val = pad_sequences(X_val, maxlen=100, padding='post', truncating='post')\n",
    "\n",
    "    X_test = tokenizer.texts_to_sequences(X_test)\n",
    "    X_test = pad_sequences(X_test, maxlen=100, padding='post', truncating='post')\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "    \"\"\"\n",
    "    X_train = vectorizer.transform(X_train).toarray()\n",
    "    X_val = vectorizer.transform(X_val).toarray()\n",
    "    X_test = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(X_val.shape)\n",
    "    print(X_test.shape)\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compile and train the model with given param\n",
    "def train_model(blueprint, tokenizer):\n",
    "    print(len(tokenizer.word_index))\n",
    "\n",
    "    global best_val_accuracy, best_model, X_train, y_train, X_val, y_val\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(tokenizer.word_index)+1, 32, input_length=100))\n",
    "    model.add(Bidirectional(LSTM(32, recurrent_dropout=0.2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    # model.add(Dense(32, activation='relu'))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    model.compile(optimizer=blueprint.optimizer(learning_rate=blueprint.learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # model.summary()\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_LR_on_plateau = ReduceLROnPlateau(patience=2, monitor='val_loss', factor=0.5)\n",
    "    history = model.fit(x=X_train, y=y_train, batch_size=blueprint.batch_size, epochs=50, verbose=2,\n",
    "                        callbacks=[early_stop, reduce_LR_on_plateau], validation_data=(X_val, y_val))\n",
    "\n",
    "    print(f'Epochs = {len(history.history[\"accuracy\"])}')\n",
    "\n",
    "    val_accuracy = max(history.history['val_accuracy'])\n",
    "\n",
    "    is_best_model = False\n",
    "\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_model = model\n",
    "\n",
    "        is_best_model = True\n",
    "\n",
    "        model.save(\"best_model.h5\")\n",
    "\n",
    "        print(f'##### Best model saved with validation accuracy: {best_val_accuracy}')\n",
    "\n",
    "    del model\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return is_best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBlueprint:\n",
    "    def __init__(self, hidden_layer_size_1, hidden_layer_size_2, activation_func, dropout, optimizer, learning_rate, batch_size):\n",
    "        self.hidden_layer_size_1 = hidden_layer_size_1\n",
    "        self.hidden_layer_size_2 = hidden_layer_size_2\n",
    "        self.activation_func = activation_func\n",
    "        self.dropout = dropout\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __str__(self):\n",
    "        str = f'hidden_layer_size_1: {self.hidden_layer_size_1}, hidden_layer_size_2: {self.hidden_layer_size_2}, activation_func: {self.activation_func}, '\n",
    "        str += f'dropout: {self.dropout}, optimizer: {self.optimizer}, learning_rate: {self.learning_rate}, batch_size: {self.batch_size}'\n",
    "\n",
    "        return str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for plot graph\n",
    "def plot_graphs(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(acc)\n",
    "    plt.plot(val_acc)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(['train accuracy', 'val accuracy'])\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(['train loss', 'val loss'])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_accuracy = 0\n",
    "best_model = None\n",
    "best_dataset = dir + 'best_dataset.csv'\n",
    "\n",
    "X = None\n",
    "y = None\n",
    "X_train = None\n",
    "y_train = None\n",
    "X_val = None\n",
    "y_val = None\n",
    "X_test = None\n",
    "y_test = None\n",
    "\n",
    "normal_blueprint = ModelBlueprint(128, 32, 'relu', 0.2, Adam, 0.0001, 36)\n",
    "\n",
    "# select best dataset first\n",
    "for i, dataset in enumerate(datasets):\n",
    "    print(i)\n",
    "    print(datetime.now(pytz.timezone('Asia/Hong_Kong')).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    load_data(dataset)\n",
    "    split_dataset(X, y)\n",
    "    tokenizer = extract_feature(CountVectorizer(), X)\n",
    "\n",
    "    if train_model(normal_blueprint, tokenizer):\n",
    "        shutil.copy2(dataset, best_dataset)\n",
    "\n",
    "    print()\n",
    "\n",
    "best_model.summary()\n",
    "\n",
    "plot_graphs(best_model.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of models\n",
    "hidden_layer_size_sample = [32, 64, 128]\n",
    "# activation_func_sample = ['relu', 'softmax', 'softplus', 'softsign', 'selu', 'elu']\n",
    "activation_func_sample = ['relu', 'softplus', 'softsign', 'selu', 'elu']\n",
    "optimizer_sample = [Adam, RMSprop]\n",
    "# learning_rate_sample = [0.00001, 0.0001, 0.001, 0.01]\n",
    "learning_rate_sample = [0.0001]\n",
    "# batch_size_sample = [12, 24, 36, 48, 60]\n",
    "batch_size_sample = [36]\n",
    "dropout_sample = [0]\n",
    "# dropout_sample = [0.1, 0.2, 0.3, 0.4]\n",
    "\n",
    "hyper_params = list(itertools.product(hidden_layer_size_sample, activation_func_sample, optimizer_sample,\n",
    "                    learning_rate_sample, batch_size_sample, dropout_sample))\n",
    "model_blueprints = [ModelBlueprint(hidden_layer_size, activation_func, optimizer, learning_rate, batch_size, dropout)\n",
    "                    for hidden_layer_size, activation_func, optimizer, learning_rate, batch_size, dropout in hyper_params]\n",
    "\n",
    "# Look at all combination of hyper_params we have\n",
    "print(len(model_blueprints))\n",
    "\n",
    "for m in model_blueprints:\n",
    "    print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function loop all hypermeter and return the best\n",
    "def train_with_all_blueprints(X_train, y_train, X_val, y_val, model_blueprints):\n",
    "    models = []\n",
    "    histories = []\n",
    "\n",
    "    for i, blueprint in enumerate(model_blueprints):\n",
    "        print(f'{i}: {blueprint}')\n",
    "\n",
    "        model, history = train_model(X_train, y_train, X_val, y_val,\n",
    "                                     blueprint.hidden_layer_size, blueprint.activation_func, blueprint.optimizer, blueprint.learning_rate,\n",
    "                                     blueprint.epochs, blueprint.batch_size, blueprint.dropout)\n",
    "\n",
    "        models.append(model)\n",
    "        histories.append(history)\n",
    "\n",
    "    return models, histories\n",
    "\n",
    "\n",
    "def get_best_model(models, histories, model_blueprints):\n",
    "    best_val_acc_overall = 0\n",
    "    best_model_index = 0\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        best_val_acc = max(model.history.history['val_accuracy'])\n",
    "\n",
    "        if (best_val_acc > best_val_acc_overall):\n",
    "            best_val_acc_overall = best_val_acc\n",
    "            best_model_index = i\n",
    "\n",
    "    return best_val_acc_overall, models[best_model_index], histories[best_model_index], model_blueprints[best_model_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test each dataset with each combination of hyperparameter\n",
    "best_models = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    X, y = load_data(dataset)\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = split_dataset(X, y)\n",
    "    X_train, X_val, X_test = extract_feature(CountVectorizer(), X, X_train, X_val, X_test)\n",
    "    models, histories = train_with_all_blueprints(X_train, y_train, X_val, y_val, model_blueprints)\n",
    "    val_accuracy, model, history, blueprint = get_best_model(models, histories, model_blueprints)\n",
    "    loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "    print(dataset)\n",
    "    print(blueprint)\n",
    "    print(f'Val accuracy: {val_accuracy}: Test accuracy: {test_accuracy}')\n",
    "\n",
    "    plot_graphs(history, dataset)\n",
    "\n",
    "    best_models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the performance of the model\n",
    "def spamDetection(message):\n",
    "    vectorizer = HashingVectorizer(stop_words='english', n_features=5000)\n",
    "    inputMsg = vectorizer.fit_transform([message]).toarray()\n",
    "    return best_model.predict(inputMsg)\n",
    "\n",
    "\n",
    "# print(spamDetection(\"hey let grab lunch tgt next week shall we\"))\n",
    "# print(spamDetection(\"important email account has been hacked attention require click link to reset password\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "87dc40f78584ad215daf1a34029e7534cfef3110b38cfe89103d23fe1936177c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
