{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\user\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.078 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import contractions\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from zhconv import convert\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "jieba.load_userdict('./jieba/dict_big.txt')\n",
    "\n",
    "\n",
    "def scToTc(text):\n",
    "    text = convert(text, 'zh-tw')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def expandContraction(text):\n",
    "    # specific\n",
    "    text = re.sub(r'i[\\'?]m', 'i am', text)\n",
    "    text = re.sub(r'let[\\'?]s', 'let us', text)\n",
    "    text = re.sub(r'don[\\'?]t', 'do not', text)\n",
    "    text = re.sub(r'can[\\'?]t', 'can not', text)\n",
    "    text = re.sub(r'won[\\'?]t', 'will not', text)\n",
    "\n",
    "    # general\n",
    "    text = re.sub(r'[\\'?]s', ' is', text)\n",
    "    text = re.sub(r'[\\'?]re', ' are', text)\n",
    "    text = re.sub(r'[\\'?]ll', ' will', text)\n",
    "    text = re.sub(r'[\\'?]d', ' would', text)\n",
    "    text = re.sub(r'[\\'?]ve', ' have', text)\n",
    "    text = re.sub(r'n[\\'?]t', ' not', text)\n",
    "\n",
    "    # library\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def cleanData(text):\n",
    "    # expand contraction\n",
    "    text = expandContraction(text)\n",
    "\n",
    "    # replace hyperlink\n",
    "    text = re.sub(r'http[s]?:\\/\\/[\\w\\/.?=-]+', ' link ', text)\n",
    "\n",
    "    # replace email address\n",
    "    text = re.sub(r'[\\w\\.+]+@[\\w\\.]+\\.[a-z]{2,}', ' email ', text)\n",
    "\n",
    "    # replace currency sign\n",
    "    text = re.sub(r'[\\$€£¥]', ' money ', text)\n",
    "\n",
    "    # replace number\n",
    "    text = re.sub(r'[\\d]+', ' number ', text)\n",
    "\n",
    "    # replace special char, other than a-z, A-Z, 0-9 and chinese\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\u4E00-\\u9FFF]+', ' ', text)\n",
    "\n",
    "    # replace new line (carriage return and line feed)\n",
    "    text = re.sub(r'[\\r\\n]', ' ', text)\n",
    "\n",
    "    # replace white space\n",
    "    text = re.sub(r'[\\s]{2,}', ' ', text)\n",
    "    text = re.sub(r'^[\\s]+|[\\s]+$', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def stopWords(text, words):\n",
    "    text = ' '.join([word for word in text.split() if word not in (words)])\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def stemming(text, stemmer):\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def lemmatization(text, lemmatizer):\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def segmentation(text):\n",
    "    text = ' '.join(jieba.cut(text))\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['netflix 無法 處理 你 的 自動 付款 你 的 帳戶 將被 禁用 co ntakhpfwqr', '親愛 的 chankwan pok 先生 或 女士 歡迎您 加入 心享 計劃 hpr 您 的 會員 編碼 為 number 初始密碼 為 number 恭喜 您 可 尊享 通過 中旅 酒店 心享 會 公眾號 推薦 朋友 加入 會員 活動 被 推薦 的 新會員 還 可以 領取 代金券 禮包 詳詢 number number number 大陸 地區 number number 港澳地區', '天外 全新 資料片 登場 開放 亞特蘭 提 斯城 推出 英雄 升變 英雄 能力 大 解放 mo alta hk 查詢 en 取消 un number', '渣打 香港 您 現正 使用 尾數 為 number 之 信用卡 在 pure intern hk l 進行 一項 網上交易 金額 hkd number number number 你 的 一次 有效 密碼 為 ngi number', '您好 我系 橙 橙 邀請 您 加細妹 微信 number 睇 朋友圈 保有 您 喜歡 嘅 哦 有 國際 大牌 l v 愛馬仕 普拉達 迪奧 聖羅蘭 古奇 芬迪 巴寶莉 阿瑪尼 寶格麗 勞力士 等 男女 服裝 鞋子 包包 手錶 皮帶 圍巾 首飾 等 第三方 擔保 交易 驗貨 滿意 再 籤 收 購物 零 風險 我要 告訴您 我 賣 嘅 系 超 a 貨 我 沒有 去 欺騙 任何人 嗰 啲 搵 我 買 嘅 人 他們 都 很 清楚']\n"
     ]
    }
   ],
   "source": [
    "input = ['[Netflix] : 無法處理你的自動付款。你的帳戶將被禁用。t.co/ntAkhpFWqR',\n",
    "         '亲爱的CHANKWAN POK先生或女士，欢迎您加入“心享”计划 HPR”，您的会员编码为：111023621183，初始密码为：006653。恭喜您可尊享通过“中旅酒店心享会”公众号推荐朋友加入会员活动，被推荐的新会员还可以领取代金券礼包！详询400-669-0000（大陆地区）、852-36040000（港澳地区）。',\n",
    "         '《天外》全新資料片登場!開放亞特蘭提斯城!推出英雄升變!英雄能力大解放! mo.alta.hk 查詢EN/取消UN81060822',\n",
    "         '渣打香港: 您現正使用尾數為0376之信用卡在Pure International HK L進行一項網上交易，金額HKD 2,038.00;你的一次有效密碼為NGY-730596。',\n",
    "         '''您好！我系橙橙\n",
    "邀請您加细妹微信：76169639 睇朋友圈保有您喜歡嘅哦！\n",
    "有國際大牌：L.V、愛馬仕、普拉達、迪奧、聖羅蘭、古奇、芬迪、巴寶莉、阿瑪尼、寶格麗、勞力士等（男女服裝、鞋子、包包、手錶、皮帶、圍巾、首飾等）第三方担保交易，驗貨滿意再签收，購物零風險\n",
    "我要告訴您：我賣嘅系超A貨，我沒有去欺騙任何人，嗰啲揾我買嘅人，他們都很清楚''']\n",
    "\n",
    "# 1, 1, 0, 0, 1\n",
    "\n",
    "\n",
    "def preprocess(input, remove_stop, stem, lemmatize):\n",
    "    words = stopwords.words('english')\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    output = []\n",
    "\n",
    "    for text in input:\n",
    "        text = str(text)\n",
    "        text = text.lower()\n",
    "        text = scToTc(text)\n",
    "        text = cleanData(text)\n",
    "\n",
    "        if remove_stop:\n",
    "            text = stopWords(text, words)\n",
    "\n",
    "        if stem:\n",
    "            text = stemming(text, stemmer)\n",
    "\n",
    "        if lemmatize:\n",
    "            text = lemmatization(text, lemmatizer)\n",
    "\n",
    "        text = segmentation(text)\n",
    "        text = re.sub(r'[\\s]{2,}', ' ', text)\n",
    "\n",
    "        output.append(text)\n",
    "    return output\n",
    "\n",
    "\n",
    "output = preprocess(input, True, True, False)\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45314\n",
      "(5, 100)\n",
      "[[17164   155   493     6     1   268   187     6     1   170 26317 17229\n",
      "   1184 26318     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [  409     1 26408 26409  1443   203  2188  1391   304 26410  1039 26411\n",
      "     14     1    35  2767   114     2 17267   114     2   822    14    15\n",
      "   1169   202 13434   845 26410   110   190    73   439   304    35    80\n",
      "    204    73     1  3694   129    13   137  1551   711   415     2     2\n",
      "      2  3911  1221     2     2 26412     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [26374  1527 26375  4040  1866 26376  1360 26377   681  9799 26378  9799\n",
      "   2024   157  9800  5911 26379  3209   169  6350   339  4873     2     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [ 7354  4494    14 32280    78  6437   114     2   745   448     8 26049\n",
      "  26274  3209  1358   147  3590 19733   647 11253     2     2     2     6\n",
      "      1   397   764   242   114     2     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [  119 17230 20555 20555   956    14 17232    22     2  7352   449 11234\n",
      "     14  1092  2566   213     7   717   687  1358  1343  8806  7999  8807\n",
      "   9792  8000  9793  9794  9795  9796  7353    43  1884  2022  3477  3587\n",
      "   1999  5643  5362  5960    43   802  1030   422  8001   855   135  6177\n",
      "   1393   453  1769  3056  2591  5117     3   851  2566  5363  1152   990\n",
      "    769     3    20   167  3208  6814 13405 11235  9797     3   177  2566\n",
      "     75   583    23   275  2185     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "def featureExtraction(input):\n",
    "    tokenizer = pickle.load(open('./best_tokenizer.pkl', 'rb'))\n",
    "\n",
    "    print(len(tokenizer.word_index))\n",
    "\n",
    "    output = input.copy()\n",
    "    output = tokenizer.texts_to_sequences(output)\n",
    "    output = pad_sequences(output, maxlen=100, padding='post', truncating='post')\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "output = featureExtraction(output)\n",
    "\n",
    "print(output.shape)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 32)           1450080   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 64)               16640     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,466,785\n",
      "Trainable params: 1,466,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[[0.9818954 ]\n",
      " [0.9998923 ]\n",
      " [0.01482893]\n",
      " [0.99958897]\n",
      " [0.999904  ]]\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('./best_model.h5')\n",
    "loaded_model.summary()\n",
    "\n",
    "pred = loaded_model.predict(output)\n",
    "true = [1, 1, 0, 0, 1]\n",
    "\n",
    "print(pred)\n",
    "\n",
    "score = accuracy_score(true, pred > 0.5)\n",
    "\n",
    "print(score)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* set01 = 0.9904858987427795\n",
    "* set02 = 0.9459459459459459\n",
    "* set03 = 0.9977263038226517\n",
    "* set04 = 0.9862816336757705\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "load_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
