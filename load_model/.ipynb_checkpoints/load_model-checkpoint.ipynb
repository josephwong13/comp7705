{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import contractions\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from zhconv import convert\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "jieba.load_userdict('./jieba/dict_big.txt')\n",
    "\n",
    "\n",
    "def scToTc(text):\n",
    "    text = convert(text, 'zh-tw')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def expandContraction(text):\n",
    "    # specific\n",
    "    text = re.sub(r'i[\\'?]m', 'i am', text)\n",
    "    text = re.sub(r'let[\\'?]s', 'let us', text)\n",
    "    text = re.sub(r'don[\\'?]t', 'do not', text)\n",
    "    text = re.sub(r'can[\\'?]t', 'can not', text)\n",
    "    text = re.sub(r'won[\\'?]t', 'will not', text)\n",
    "\n",
    "    # general\n",
    "    text = re.sub(r'[\\'?]s', ' is', text)\n",
    "    text = re.sub(r'[\\'?]re', ' are', text)\n",
    "    text = re.sub(r'[\\'?]ll', ' will', text)\n",
    "    text = re.sub(r'[\\'?]d', ' would', text)\n",
    "    text = re.sub(r'[\\'?]ve', ' have', text)\n",
    "    text = re.sub(r'n[\\'?]t', ' not', text)\n",
    "\n",
    "    # library\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def cleanData(text):\n",
    "    # expand contraction\n",
    "    text = expandContraction(text)\n",
    "\n",
    "    # replace hyperlink\n",
    "    text = re.sub(r'http[s]?:\\/\\/[\\w\\/.?=-]+', ' link ', text)\n",
    "\n",
    "    # replace email address\n",
    "    text = re.sub(r'[\\w\\.+]+@[\\w\\.]+\\.[a-z]{2,}', ' email ', text)\n",
    "\n",
    "    # replace currency sign\n",
    "    text = re.sub(r'[\\$€£¥]', ' money ', text)\n",
    "\n",
    "    # replace number\n",
    "    text = re.sub(r'[\\d]+', ' number ', text)\n",
    "\n",
    "    # replace special char, other than a-z, A-Z, 0-9 and chinese\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\u4E00-\\u9FFF]+', ' ', text)\n",
    "\n",
    "    # replace new line (carriage return and line feed)\n",
    "    text = re.sub(r'[\\r\\n]', ' ', text)\n",
    "\n",
    "    # replace white space\n",
    "    text = re.sub(r'[\\s]{2,}', ' ', text)\n",
    "    text = re.sub(r'^[\\s]+|[\\s]+$', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def stopWords(text, words):\n",
    "    text = ' '.join([word for word in text.split() if word not in (words)])\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def stemming(text, stemmer):\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def lemmatization(text, lemmatizer):\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def segmentation(text):\n",
    "    text = ' '.join(jieba.cut(text))\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = ['[Netflix] : 無法處理你的自動付款。你的帳戶將被禁用。t.co/ntAkhpFWqR',\n",
    "         '亲爱的CHANKWAN POK先生或女士，欢迎您加入“心享”计划 HPR”，您的会员编码为：111023621183，初始密码为：006653。恭喜您可尊享通过“中旅酒店心享会”公众号推荐朋友加入会员活动，被推荐的新会员还可以领取代金券礼包！详询400-669-0000（大陆地区）、852-36040000（港澳地区）。',\n",
    "         '《天外》全新資料片登場!開放亞特蘭提斯城!推出英雄升變!英雄能力大解放! mo.alta.hk 查詢EN/取消UN81060822',\n",
    "         '渣打香港: 您現正使用尾數為0376之信用卡在Pure International HK L進行一項網上交易，金額HKD 2,038.00;你的一次有效密碼為NGY-730596。',\n",
    "         '''您好！我系橙橙\n",
    "邀請您加细妹微信：76169639 睇朋友圈保有您喜歡嘅哦！\n",
    "有國際大牌：L.V、愛馬仕、普拉達、迪奧、聖羅蘭、古奇、芬迪、巴寶莉、阿瑪尼、寶格麗、勞力士等（男女服裝、鞋子、包包、手錶、皮帶、圍巾、首飾等）第三方担保交易，驗貨滿意再签收，購物零風險\n",
    "我要告訴您：我賣嘅系超A貨，我沒有去欺騙任何人，嗰啲揾我買嘅人，他們都很清楚''']\n",
    "\n",
    "# 1, 1, 0, 0, 1\n",
    "\n",
    "\n",
    "def preprocess(input, remove_stop, stem, lemmatize):\n",
    "    words = stopwords.words('english')\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    output = []\n",
    "\n",
    "    for text in input:\n",
    "        text = text.lower()\n",
    "        text = scToTc(text)\n",
    "        text = cleanData(text)\n",
    "\n",
    "        if remove_stop:\n",
    "            text = stopWords(text, words)\n",
    "\n",
    "        if stem:\n",
    "            text = stemming(text, stemmer)\n",
    "\n",
    "        if lemmatize:\n",
    "            text = lemmatization(text, lemmatizer)\n",
    "\n",
    "        text = segmentation(text)\n",
    "        text = re.sub(r'[\\s]{2,}', ' ', text)\n",
    "\n",
    "        output.append(text)\n",
    "    return output\n",
    "\n",
    "\n",
    "output = preprocess(input, True, True, True)\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    df = pd.read_csv(filename, header=None, encoding='utf-8').dropna()\n",
    "    df.columns = ['label', 'data']\n",
    "\n",
    "    return df['data']\n",
    "\n",
    "\n",
    "def featureExtraction(input):\n",
    "    X = load_data('../dataset/set_01_02_03_04_0_0_0_new.csv')\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(X)\n",
    "\n",
    "    print(vectorizer.get_feature_names_out())\n",
    "\n",
    "    output = input.copy()\n",
    "    output = vectorizer.transform(output).toarray()\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "output = featureExtraction(output)\n",
    "\n",
    "print(output.shape)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('./best_model.h5')\n",
    "loaded_model.summary()\n",
    "\n",
    "pred = loaded_model.predict(output)\n",
    "true = [1, 1, 0, 0, 1]\n",
    "\n",
    "print(pred)\n",
    "\n",
    "score = accuracy_score(true, pred > 0.5)\n",
    "\n",
    "print(score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "load_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
