{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.1\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('../dataset/set_01_02_03_04_0_0_0_new.csv'),\n",
       " WindowsPath('../dataset/set_01_02_03_04_0_0_1_new.csv'),\n",
       " WindowsPath('../dataset/set_01_02_03_04_0_1_0_new.csv'),\n",
       " WindowsPath('../dataset/set_01_02_03_04_0_1_1_new.csv'),\n",
       " WindowsPath('../dataset/set_01_02_03_04_1_0_0_new.csv'),\n",
       " WindowsPath('../dataset/set_01_02_03_04_1_0_1_new.csv'),\n",
       " WindowsPath('../dataset/set_01_02_03_04_1_1_0_new.csv'),\n",
       " WindowsPath('../dataset/set_01_02_03_04_1_1_1_new.csv')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define list of dataset\n",
    "dir = '../dataset/'\n",
    "datasets = [file for file in Path(dir).glob('*.csv') if not file.name == \"best_dataset.csv\"]\n",
    "\n",
    "datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load data\n",
    "def load_data(filename):\n",
    "    print(filename)\n",
    "\n",
    "    df = pd.read_csv(filename, header=None, encoding='utf-8').dropna()\n",
    "    df.columns = ['label', 'data']\n",
    "    df = df[:2]\n",
    "\n",
    "    global X, y\n",
    "\n",
    "    X = df['data']\n",
    "    y = df['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\dataset\\set_01_02_03_04_0_0_0_new.csv\n",
      "['attempt' 'await' 'before' 'box' 'call' 'callcost' 'cash' 'collection'\n",
      " 'complimentary' 'contact' 'cs' 'csbcm' 'from' 'have' 'holiday' 'hp' 'is'\n",
      " 'landline' 'max' 'mobilesvary' 'nd' 'number' 'or' 'ppm' 'sae' 'tenerife'\n",
      " 'the' 'this' 'to' 'urgent' 'wc' 'won' 'xx' 'yf' 'you' 'your']\n",
      "[[ 0  1  0  1  1  0  1  1  1  0  1  0  1  0  1  1  0  1  0  0  0  9  1  1\n",
      "   1  1  0  0  0  1  0  0  0  1  0  1]\n",
      " [ 1  0  1  0  1  1  0  0  0  1  0  1  0  1  0  0  1  0  1  1  1 11  0  1\n",
      "   0  0  1  1  1  1  1  1  1  0  2  0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = None\n",
    "y = None\n",
    "\n",
    "load_data(datasets[0])\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X)\n",
    "\n",
    "X = vectorizer.transform(X).toarray()\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\dataset\\set_01_02_03_04_0_0_0_new.csv\n",
      "['attempt' 'await' 'before' 'box' 'call' 'callcost' 'cash' 'collection'\n",
      " 'complimentary' 'contact' 'cs' 'csbcm' 'from' 'have' 'holiday' 'hp' 'is'\n",
      " 'landline' 'max' 'mobilesvary' 'nd' 'number' 'or' 'ppm' 'sae' 'tenerife'\n",
      " 'the' 'this' 'to' 'urgent' 'wc' 'won' 'xx' 'yf' 'you' 'your']\n",
      "[[0.         0.13184802 0.         0.13184802 0.09381095 0.\n",
      "  0.13184802 0.13184802 0.13184802 0.         0.13184802 0.\n",
      "  0.13184802 0.         0.13184802 0.13184802 0.         0.13184802\n",
      "  0.         0.         0.         0.84429854 0.13184802 0.09381095\n",
      "  0.13184802 0.13184802 0.         0.         0.         0.09381095\n",
      "  0.         0.         0.         0.13184802 0.         0.13184802]\n",
      " [0.10991384 0.         0.10991384 0.         0.0782046  0.10991384\n",
      "  0.         0.         0.         0.10991384 0.         0.10991384\n",
      "  0.         0.10991384 0.         0.         0.10991384 0.\n",
      "  0.10991384 0.10991384 0.10991384 0.8602506  0.         0.0782046\n",
      "  0.         0.         0.10991384 0.10991384 0.10991384 0.0782046\n",
      "  0.10991384 0.10991384 0.10991384 0.         0.21982767 0.        ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "load_data(datasets[0])\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X)\n",
    "\n",
    "X = vectorizer.transform(X).toarray()\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\dataset\\set_01_02_03_04_0_0_0_new.csv\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "load_data(datasets[0])\n",
    "vectorizer = HashingVectorizer()\n",
    "vectorizer.fit(X)\n",
    "\n",
    "X = vectorizer.transform(X).toarray()\n",
    "print(X)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\dataset\\set_01_02_03_04_0_0_0_new.csv\n",
      "{'number': 1, 'urgent': 2, 'call': 3, 't': 4, 'ppm': 5, 'you': 6, 'from': 7, 'landline': 8, 'your': 9, 'complimentary': 10, 'tenerife': 11, 'holiday': 12, 'or': 13, 'cash': 14, 'await': 15, 'collection': 16, 'sae': 17, 'cs': 18, 'box': 19, 'hp': 20, 'yf': 21, 'this': 22, 'is': 23, 'the': 24, 'nd': 25, 'attempt': 26, 'to': 27, 'contact': 28, 'have': 29, 'won': 30, 'before': 31, 'csbcm': 32, 'wc': 33, 'n': 34, 'xx': 35, 'callcost': 36, 'mobilesvary': 37, 'max': 38}\n",
      "[[ 2  3  1  7  8  9 10  1 11 12 13  1  1 14 15 16 17  4 18 19  1 20  1  1\n",
      "  21  1  5  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0]\n",
      " [ 1  2 22 23 24  1 25 26 27 28  6  6 29 30  1  3  1 31  1  4 32  1 33  1\n",
      "  34  1 35 36  1  5 37 38  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "load_data(datasets[0])\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(X, maxlen=100, padding='post', truncating='post')\n",
    "print(tokenizer.word_index)\n",
    "print(X)\n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "87dc40f78584ad215daf1a34029e7534cfef3110b38cfe89103d23fe1936177c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
