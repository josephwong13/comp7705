{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 22:51:02.689105: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from tensorflow import keras\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define list of dataset\n",
    "datasets = [\n",
    "    # 'set_01_02_03_0_0_0_new.csv',\n",
    "    # 'set_01_02_03_0_0_1_new.csv',\n",
    "    # 'set_01_02_03_0_1_0_new.csv',\n",
    "    # 'set_01_02_03_0_1_1_new.csv',\n",
    "    # 'set_01_02_03_1_0_0_new.csv',\n",
    "    # 'set_01_02_03_1_0_1_new.csv',\n",
    "    # 'set_01_02_03_1_1_0_new.csv',\n",
    "    'set_01_02_03_04_1_1_1_new.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function to load data\n",
    "def load_data(filename):\n",
    "    df = pd.read_csv(f'../dataset/{filename}', encoding='utf-8').dropna()\n",
    "    df.columns = ['label', 'data']\n",
    "    return df['data'], df['label']\n",
    "\n",
    "# Helper function to create train val test split\n",
    "def split_dataset(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.33, random_state=7)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# Helper function to extract feature\n",
    "def extract_feature(X_train, X_val, X_test):\n",
    "    vectorizer = HashingVectorizer(stop_words='english', n_features=50000)\n",
    "    X_train_extract = vectorizer.fit_transform(X_train).toarray()\n",
    "    X_val_extract = vectorizer.transform(X_val).toarray()\n",
    "    X_test_extract = vectorizer.transform(X_test).toarray()\n",
    "    return X_train_extract, X_val_extract, X_test_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function to compile and train the model with given param\n",
    "def train_model(X_train, y_train, X_val, y_val, learning_rate, hidden_layer_size, optimizer, epochs):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=hidden_layer_size, activation='relu', input_dim=X_train.shape[1]))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=32, validation_data=(X_val, y_val))\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "learning_rate: 0.0001, hidden_layer_size: 32, optimizer: <class 'keras.optimizers.optimizer_experimental.adam.Adam'>, epoch: $10\n",
      "learning_rate: 0.0001, hidden_layer_size: 32, optimizer: <class 'keras.optimizers.optimizer_experimental.rmsprop.RMSprop'>, epoch: $10\n",
      "learning_rate: 0.0001, hidden_layer_size: 64, optimizer: <class 'keras.optimizers.optimizer_experimental.adam.Adam'>, epoch: $10\n",
      "learning_rate: 0.0001, hidden_layer_size: 64, optimizer: <class 'keras.optimizers.optimizer_experimental.rmsprop.RMSprop'>, epoch: $10\n",
      "learning_rate: 0.0001, hidden_layer_size: 128, optimizer: <class 'keras.optimizers.optimizer_experimental.adam.Adam'>, epoch: $10\n",
      "learning_rate: 0.0001, hidden_layer_size: 128, optimizer: <class 'keras.optimizers.optimizer_experimental.rmsprop.RMSprop'>, epoch: $10\n",
      "learning_rate: 0.001, hidden_layer_size: 32, optimizer: <class 'keras.optimizers.optimizer_experimental.adam.Adam'>, epoch: $10\n",
      "learning_rate: 0.001, hidden_layer_size: 32, optimizer: <class 'keras.optimizers.optimizer_experimental.rmsprop.RMSprop'>, epoch: $10\n",
      "learning_rate: 0.001, hidden_layer_size: 64, optimizer: <class 'keras.optimizers.optimizer_experimental.adam.Adam'>, epoch: $10\n",
      "learning_rate: 0.001, hidden_layer_size: 64, optimizer: <class 'keras.optimizers.optimizer_experimental.rmsprop.RMSprop'>, epoch: $10\n",
      "learning_rate: 0.001, hidden_layer_size: 128, optimizer: <class 'keras.optimizers.optimizer_experimental.adam.Adam'>, epoch: $10\n",
      "learning_rate: 0.001, hidden_layer_size: 128, optimizer: <class 'keras.optimizers.optimizer_experimental.rmsprop.RMSprop'>, epoch: $10\n",
      "learning_rate: 0.01, hidden_layer_size: 32, optimizer: <class 'keras.optimizers.optimizer_experimental.adam.Adam'>, epoch: $10\n",
      "learning_rate: 0.01, hidden_layer_size: 32, optimizer: <class 'keras.optimizers.optimizer_experimental.rmsprop.RMSprop'>, epoch: $10\n",
      "learning_rate: 0.01, hidden_layer_size: 64, optimizer: <class 'keras.optimizers.optimizer_experimental.adam.Adam'>, epoch: $10\n",
      "learning_rate: 0.01, hidden_layer_size: 64, optimizer: <class 'keras.optimizers.optimizer_experimental.rmsprop.RMSprop'>, epoch: $10\n",
      "learning_rate: 0.01, hidden_layer_size: 128, optimizer: <class 'keras.optimizers.optimizer_experimental.adam.Adam'>, epoch: $10\n",
      "learning_rate: 0.01, hidden_layer_size: 128, optimizer: <class 'keras.optimizers.optimizer_experimental.rmsprop.RMSprop'>, epoch: $10\n"
     ]
    }
   ],
   "source": [
    "class ModelBlueprint:\n",
    "    def __init__(self, learning_rate, hidden_layer_size, optimizer, epochs):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "    def __str__(self):\n",
    "        return f'learning_rate: {self.learning_rate}, hidden_layer_size: {self.hidden_layer_size}, optimizer: {self.optimizer}, epoch: ${self.epochs}'\n",
    "\n",
    "# Define a list of models\n",
    "modelBlueprints = []\n",
    "lr_sample = [0.0001, 0.001, 0.01]\n",
    "model_size_sample = [32, 64, 128]\n",
    "epoch = [10]\n",
    "optimizer_sample = [keras.optimizers.Adam, keras.optimizers.RMSprop]\n",
    "\n",
    "# lr_sample = [0.001]\n",
    "# model_size_sample = [32]\n",
    "# optimizer_sample = [keras.optimizers.Adam, keras.optimizers.RMSprop]\n",
    "# epoch = [4]\n",
    "\n",
    "hyperparams = list(itertools.product(lr_sample, model_size_sample, optimizer_sample, epoch))\n",
    "modelBlueprints = [ModelBlueprint(lr, model_size, optimizer, epoch) for lr, model_size, optimizer, epoch in hyperparams]\n",
    "\n",
    "# Look at all combination of hyperparams we have\n",
    "print(len(modelBlueprints))\n",
    "for m in modelBlueprints:\n",
    "    print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function loop all hypermeter and return the best\n",
    "def train_with_all_blueprints(dataset, X_train, y_train, X_val, y_val, modelBlueprints):\n",
    "    models = []\n",
    "    histories = []\n",
    "    for blueprint in modelBlueprints:\n",
    "        model, history = train_model(X_train, y_train, X_val, y_val, blueprint.learning_rate, blueprint.hidden_layer_size, blueprint.optimizer, blueprint.epochs)\n",
    "        models.append(model)\n",
    "        histories.append(history)\n",
    "    return models, histories\n",
    "\n",
    "def get_best_model(models, histories, modelBlueprints):\n",
    "    best_val_acc_overall = 0\n",
    "    best_model_index = 0\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        best_val_acc = max(model.history.history['val_accuracy'])\n",
    "        if (best_val_acc > best_val_acc_overall):\n",
    "            best_val_acc_overall = best_val_acc\n",
    "            best_model_index = i\n",
    "\n",
    "    return best_val_acc_overall, models[i], histories[i], modelBlueprints[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function for plot graph\n",
    "def plot_graphs(history, metric, dataset, modelSummary):\n",
    "  plt.suptitle(modelSummary)\n",
    "  plt.title(f\"{dataset} History\")\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n",
    "  plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 22:51:35.344915: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-24 22:51:35.356666: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-24 22:51:35.357062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-24 22:51:35.357960: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-24 22:51:35.358402: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-24 22:51:35.358872: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-24 22:51:35.359181: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-24 22:51:36.895721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-24 22:51:36.896232: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-24 22:51:36.896570: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-24 22:51:36.896867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 328 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:05:00.0, compute capability: 7.5\n",
      "2023-06-24 22:52:36.824958: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 4616200000 exceeds 10% of free system memory.\n",
      "2023-06-24 22:52:51.509009: W tensorflow/tsl/framework/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.30GiB (rounded to 4616200192)requested by op _EagerConst\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-06-24 22:52:51.509093: I tensorflow/tsl/framework/bfc_allocator.cc:1034] BFCAllocator dump for GPU_0_bfc\n",
      "2023-06-24 22:52:51.509110: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (256): \tTotal Chunks: 13, Chunks in use: 13. 3.2KiB allocated for chunks. 3.2KiB in use in bin. 308B client-requested in use in bin.\n",
      "2023-06-24 22:52:51.509124: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-24 22:52:51.509135: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2023-06-24 22:52:51.509145: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-24 22:52:51.509154: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-24 22:52:51.509164: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-24 22:52:51.509173: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-24 22:52:51.509182: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-24 22:52:51.509192: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-24 22:52:51.509201: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-24 22:52:51.509210: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-24 22:52:51.509219: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-24 22:52:51.509229: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-24 22:52:51.509238: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-24 22:52:51.509248: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4194304): \tTotal Chunks: 1, Chunks in use: 1. 6.10MiB allocated for chunks. 6.10MiB in use in bin. 6.10MiB client-requested in use in bin.\n",
      "2023-06-24 22:52:51.509260: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8388608): \tTotal Chunks: 1, Chunks in use: 0. 12.21MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-24 22:52:51.509269: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-24 22:52:51.509278: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-24 22:52:51.509292: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-24 22:52:51.509301: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-24 22:52:51.509312: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (268435456): \tTotal Chunks: 1, Chunks in use: 0. 310.25MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-24 22:52:51.509330: I tensorflow/tsl/framework/bfc_allocator.cc:1057] Bin for 4.30GiB was 256.00MiB, Chunk State: \n",
      "2023-06-24 22:52:51.509346: I tensorflow/tsl/framework/bfc_allocator.cc:1063]   Size: 310.25MiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 6.10MiB | Requested Size: 6.10MiB | in_use: 1 | bin_num: -1\n",
      "2023-06-24 22:52:51.509355: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 344522752\n",
      "2023-06-24 22:52:51.509372: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 15003e000000 of size 256 next 1\n",
      "2023-06-24 22:52:51.509381: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 15003e000100 of size 1280 next 2\n",
      "2023-06-24 22:52:51.509391: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 15003e000600 of size 256 next 3\n",
      "2023-06-24 22:52:51.509399: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 15003e000700 of size 256 next 4\n",
      "2023-06-24 22:52:51.509408: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 15003e000800 of size 256 next 6\n",
      "2023-06-24 22:52:51.509416: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 15003e000900 of size 256 next 7\n",
      "2023-06-24 22:52:51.509425: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 15003e000a00 of size 256 next 5\n",
      "2023-06-24 22:52:51.509434: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 15003e000b00 of size 256 next 8\n",
      "2023-06-24 22:52:51.509443: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 15003e000c00 of size 256 next 12\n",
      "2023-06-24 22:52:51.509451: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 15003e000d00 of size 256 next 11\n",
      "2023-06-24 22:52:51.509460: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 15003e000e00 of size 256 next 13\n",
      "2023-06-24 22:52:51.509469: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 15003e000f00 of size 256 next 14\n",
      "2023-06-24 22:52:51.509477: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 15003e001000 of size 256 next 15\n",
      "2023-06-24 22:52:51.509486: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 15003e001100 of size 256 next 16\n",
      "2023-06-24 22:52:51.509495: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 15003e001200 of size 12798208 next 9\n",
      "2023-06-24 22:52:51.509504: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 15003ec35b00 of size 6400000 next 10\n",
      "2023-06-24 22:52:51.509513: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 15003f250300 of size 325319936 next 18446744073709551615\n",
      "2023-06-24 22:52:51.509522: I tensorflow/tsl/framework/bfc_allocator.cc:1095]      Summary of in-use Chunks by size: \n",
      "2023-06-24 22:52:51.509532: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 13 Chunks of size 256 totalling 3.2KiB\n",
      "2023-06-24 22:52:51.509559: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2023-06-24 22:52:51.509574: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 6400000 totalling 6.10MiB\n",
      "2023-06-24 22:52:51.509584: I tensorflow/tsl/framework/bfc_allocator.cc:1102] Sum Total of in-use chunks: 6.11MiB\n",
      "2023-06-24 22:52:51.509593: I tensorflow/tsl/framework/bfc_allocator.cc:1104] total_region_allocated_bytes_: 344522752 memory_limit_: 344522752 available bytes: 0 curr_region_allocation_bytes_: 689045504\n",
      "2023-06-24 22:52:51.509627: I tensorflow/tsl/framework/bfc_allocator.cc:1110] Stats: \n",
      "Limit:                       344522752\n",
      "InUse:                         6404608\n",
      "MaxInUse:                     19202560\n",
      "NumAllocs:                          25\n",
      "MaxAllocSize:                  6400000\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-06-24 22:52:51.509666: W tensorflow/tsl/framework/bfc_allocator.cc:492] *__***______________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m X_train, y_train, X_val, y_val, X_test, y_test \u001b[38;5;241m=\u001b[39m split_dataset(X, y)\n\u001b[1;32m      7\u001b[0m X_train, X_val, X_test \u001b[38;5;241m=\u001b[39m extract_feature(X_train, X_val, X_test)\n\u001b[0;32m----> 8\u001b[0m models, histories \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_with_all_blueprints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelBlueprints\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m val_accuracy, model, history, blueprint \u001b[38;5;241m=\u001b[39m get_best_model(models, histories, modelBlueprints)\n\u001b[1;32m     10\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36mtrain_with_all_blueprints\u001b[0;34m(dataset, X_train, y_train, X_val, y_val, modelBlueprints)\u001b[0m\n\u001b[1;32m      4\u001b[0m histories \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blueprint \u001b[38;5;129;01min\u001b[39;00m modelBlueprints:\n\u001b[0;32m----> 6\u001b[0m     model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblueprint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblueprint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_layer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblueprint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblueprint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     models\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m      8\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(history)\n",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(X_train, y_train, X_val, y_val, learning_rate, hidden_layer_size, optimizer, epochs)\u001b[0m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 7\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_2_11/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_2_11/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "# Test each dataset with each combination of hyperperimeter \n",
    "best_models = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    X, y = load_data(dataset)\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = split_dataset(X, y)\n",
    "    X_train, X_val, X_test = extract_feature(X_train, X_val, X_test)\n",
    "    models, histories = train_with_all_blueprints(dataset, X_train, y_train, X_val, y_val, modelBlueprints)\n",
    "    val_accuracy, model, history, blueprint = get_best_model(models, histories, modelBlueprints)\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f'Dataset: {dataset}: Best Val accuracy: {val_accuracy} under model: {blueprint}')\n",
    "    print(f'Dataset: {dataset}: Test accuracy: {accuracy} under model: {blueprint}')\n",
    "    plot_graphs(history, 'accuracy', dataset, blueprint)\n",
    "    best_models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "[[0.00272815]]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[[0.10677119]]\n"
     ]
    }
   ],
   "source": [
    "# Test the performance of the model\n",
    "def spamDetection(message):\n",
    "    vectorizer = HashingVectorizer(stop_words='english', n_features=5000)\n",
    "    inputMsg = vectorizer.fit_transform([message]).toarray()\n",
    "    return best_model.predict(inputMsg)\n",
    "\n",
    "print(spamDetection(\"hey let grab lunch tgt next week shall we\"))\n",
    "print(spamDetection(\"important email account has been hacked attention require click link to reset password\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "87dc40f78584ad215daf1a34029e7534cfef3110b38cfe89103d23fe1936177c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
